{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/badumetsihlongwane-jpg/AgentShell/blob/main/nexus_fx_consolidated2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87426a67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "87426a67",
    "outputId": "f82756d6-f3e9-4270-b70b-bf56f5cd287d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\nNEXUS-FX: Nested Exchange Universal Sequencer for ForeX\\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\nConsolidated Single-File Version for Google Colab / Kaggle\\n\\nThis file consolidates the entire NEXUS-FX codebase into a single importable module\\nfor easy use in notebook environments like Google Colab and Kaggle.\\n\\nNEXUS-FX is a nested associative memory architecture for forex trading that combines:\\n- Self-Modifying Titans for in-context adaptive learning\\n- Continuum Memory System for multi-timescale knowledge hierarchy\\n- Cross-Pair Memory for currency correlation learning  \\n- Session-aware frequency gating\\n- Regime detection\\n- Multi-task prediction heads\\n\\nTo use this file:\\n    1. Upload it to your Colab/Kaggle environment\\n    2. Import: `import nexus_fx_consolidated as nfx`\\n    3. Create model: `model = nfx.NEXUSFX(nfx.NexusFXConfig())`\\n\\nOriginal repository: https://github.com/badumetsihlongwane-jpg/Fx\\n\\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "NEXUS-FX: Nested Exchange Universal Sequencer for ForeX\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "Consolidated Single-File Version for Google Colab / Kaggle\n",
    "\n",
    "This file consolidates the entire NEXUS-FX codebase into a single importable module\n",
    "for easy use in notebook environments like Google Colab and Kaggle.\n",
    "\n",
    "NEXUS-FX is a nested associative memory architecture for forex trading that combines:\n",
    "- Self-Modifying Titans for in-context adaptive learning\n",
    "- Continuum Memory System for multi-timescale knowledge hierarchy\n",
    "- Cross-Pair Memory for currency correlation learning\n",
    "- Session-aware frequency gating\n",
    "- Regime detection\n",
    "- Multi-task prediction heads\n",
    "\n",
    "To use this file:\n",
    "    1. Upload it to your Colab/Kaggle environment\n",
    "    2. Import: `import nexus_fx_consolidated as nfx`\n",
    "    3. Create model: `model = nfx.NEXUSFX(nfx.NexusFXConfig())`\n",
    "\n",
    "Original repository: https://github.com/badumetsihlongwane-jpg/Fx\n",
    "\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edeb12",
   "metadata": {
    "id": "90edeb12"
   },
   "source": [
    "============================================================================\n",
    "IMPORTS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b9bfee",
   "metadata": {
    "id": "c4b9bfee"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed75ee2a",
   "metadata": {
    "id": "ed75ee2a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bab574",
   "metadata": {
    "id": "66bab574"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b35d090",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "9b35d090",
    "outputId": "3d6913bd-5a62-478e-b714-676af0a0f8e3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nLogging utilities for NEXUS-FX.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# ============= UTILITIES - LOGGING =============\n",
    "\"\"\"\n",
    "Logging utilities for NEXUS-FX.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9511a1a",
   "metadata": {
    "id": "e9511a1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1304ae",
   "metadata": {
    "id": "4f1304ae"
   },
   "outputs": [],
   "source": [
    "def setup_logger(name: str = 'nexus_fx', level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logger with consistent formatting.\n",
    "\n",
    "    Args:\n",
    "        name: Logger name\n",
    "        level: Logging level\n",
    "\n",
    "    Returns:\n",
    "        Configured logger\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Console handler\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    handler.setLevel(level)\n",
    "\n",
    "    # Formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a74dbe",
   "metadata": {
    "id": "57a74dbe"
   },
   "outputs": [],
   "source": [
    "class MetricsLogger:\n",
    "    \"\"\"\n",
    "    Logs training metrics to file and/or console.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_file: str = 'metrics.jsonl'):\n",
    "        self.log_file = log_file\n",
    "        self.logger = setup_logger('metrics')\n",
    "\n",
    "    def log(self, metrics: Dict[str, Any], step: int, epoch: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Log metrics.\n",
    "\n",
    "        Args:\n",
    "            metrics: Dictionary of metric name -> value\n",
    "            step: Global step number\n",
    "            epoch: Epoch number\n",
    "        \"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'step': step,\n",
    "            'epoch': epoch,\n",
    "            **metrics\n",
    "        }\n",
    "\n",
    "        # Write to file\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "\n",
    "        # Log to console\n",
    "        metrics_str = ', '.join(f'{k}: {v:.4f}' for k, v in metrics.items() if isinstance(v, (int, float)))\n",
    "        self.logger.info(f\"Step {step} - {metrics_str}\")\n",
    "\n",
    "    def log_summary(self, summary: str) -> None:\n",
    "        \"\"\"Log a summary string\"\"\"\n",
    "        self.logger.info(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea7a616",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7ea7a616",
    "outputId": "3da425c3-f76c-4134-f5f9-5be40fa68157"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nUtility functions for market analysis.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# ============= UTILITIES - MARKET =============\n",
    "\"\"\"\n",
    "Utility functions for market analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35fd432",
   "metadata": {
    "id": "a35fd432"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd58364",
   "metadata": {
    "id": "5dd58364"
   },
   "outputs": [],
   "source": [
    "def get_active_sessions(dt: datetime) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get list of active forex sessions for a given datetime.\n",
    "\n",
    "    Args:\n",
    "        dt: Datetime in UTC\n",
    "\n",
    "    Returns:\n",
    "        List of active session names\n",
    "    \"\"\"\n",
    "    hour = dt.hour\n",
    "    active = []\n",
    "\n",
    "    # Sydney: 22:00-07:00 GMT\n",
    "    if hour >= 22 or hour < 7:\n",
    "        active.append('sydney')\n",
    "\n",
    "    # Tokyo: 00:00-09:00 GMT\n",
    "    if hour < 9:\n",
    "        active.append('tokyo')\n",
    "\n",
    "    # London: 08:00-17:00 GMT\n",
    "    if 8 <= hour < 17:\n",
    "        active.append('london')\n",
    "\n",
    "    # New York: 13:00-22:00 GMT\n",
    "    if 13 <= hour < 22:\n",
    "        active.append('new_york')\n",
    "\n",
    "    return active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2203d4d1",
   "metadata": {
    "id": "2203d4d1"
   },
   "outputs": [],
   "source": [
    "def is_market_open(dt: datetime) -> bool:\n",
    "    \"\"\"Check if forex market is open\"\"\"\n",
    "    # Forex is open 24/5\n",
    "    weekday = dt.weekday()\n",
    "    return weekday < 5  # Monday-Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73d1a32",
   "metadata": {
    "id": "d73d1a32"
   },
   "outputs": [],
   "source": [
    "def calculate_spread(pair: str, session: str = 'london') -> float:\n",
    "    \"\"\"\n",
    "    Estimate typical spread for a currency pair.\n",
    "\n",
    "    Args:\n",
    "        pair: Currency pair (e.g., 'EURUSD')\n",
    "        session: Trading session\n",
    "\n",
    "    Returns:\n",
    "        Spread in pips\n",
    "    \"\"\"\n",
    "    # Typical spreads (in pips)\n",
    "    base_spreads = {\n",
    "        'EURUSD': 0.8,\n",
    "        'GBPUSD': 1.0,\n",
    "        'USDJPY': 0.9,\n",
    "        'AUDUSD': 1.2,\n",
    "    }\n",
    "\n",
    "    spread = base_spreads.get(pair, 2.0)\n",
    "\n",
    "    # Wider spreads during off-hours\n",
    "    if session in ['sydney', 'tokyo']:\n",
    "        spread *= 1.5\n",
    "\n",
    "    return spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf0d3ac3",
   "metadata": {
    "id": "cf0d3ac3"
   },
   "outputs": [],
   "source": [
    "def detect_session(hour: int) -> str:\n",
    "    \"\"\"\n",
    "    Detect primary forex session for given hour.\n",
    "\n",
    "    Args:\n",
    "        hour: Hour in GMT (0-23)\n",
    "\n",
    "    Returns:\n",
    "        Primary session name\n",
    "    \"\"\"\n",
    "    if 8 <= hour < 13:\n",
    "        return 'london'\n",
    "    elif 13 <= hour < 22:\n",
    "        return 'new_york'\n",
    "    elif hour < 9:\n",
    "        return 'tokyo'\n",
    "    else:\n",
    "        return 'sydney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24e274a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "a24e274a",
    "outputId": "12742f40-f20f-49ca-9fdc-ae4f9adf434f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nConfiguration module for NEXUS-FX.\\n\\nAll hyperparameters and model configurations are defined here using\\nPython dataclasses for type safety and easy serialization.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# ============= CONFIGURATION =============\n",
    "\"\"\"\n",
    "Configuration module for NEXUS-FX.\n",
    "\n",
    "All hyperparameters and model configurations are defined here using\n",
    "Python dataclasses for type safety and easy serialization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4a8d72",
   "metadata": {
    "id": "aa4a8d72"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a92783",
   "metadata": {
    "id": "46a92783"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NexusFXConfig:\n",
    "    \"\"\"\n",
    "    Main configuration for the NEXUS-FX model.\n",
    "\n",
    "    This configuration defines all hyperparameters for the nested associative\n",
    "    memory architecture, including memory dimensions, update frequencies,\n",
    "    and training parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # ========== Model Dimensions ==========\n",
    "    input_dim: int = 64\n",
    "    \"\"\"Base input dimension after feature encoding\"\"\"\n",
    "\n",
    "    hidden_dim: int = 256\n",
    "    \"\"\"Hidden dimension for all internal representations\"\"\"\n",
    "\n",
    "    num_memory_slots: int = 128\n",
    "    \"\"\"Number of key-value slots in each associative memory\"\"\"\n",
    "\n",
    "    num_titans_layers: int = 4\n",
    "    \"\"\"Number of Self-Modifying Titans layers\"\"\"\n",
    "\n",
    "    # ========== Continuum Memory System (CMS) ==========\n",
    "    num_cms_levels: int = 4\n",
    "    \"\"\"Number of memory levels in the continuum (different timescales)\"\"\"\n",
    "\n",
    "    cms_base_frequency: int = 1\n",
    "    \"\"\"Base update frequency (fastest level, updates every step)\"\"\"\n",
    "\n",
    "    cms_frequency_multiplier: int = 10\n",
    "    \"\"\"Multiplier between adjacent memory levels (exponential scaling)\"\"\"\n",
    "\n",
    "    cms_hidden_dims: Optional[List[int]] = None\n",
    "    \"\"\"Hidden dimensions for each CMS level (default: all use hidden_dim)\"\"\"\n",
    "\n",
    "    # ========== Cross-Pair Memory ==========\n",
    "    num_pairs: int = 4\n",
    "    \"\"\"Number of currency pairs in the dataset\"\"\"\n",
    "\n",
    "    num_correlation_slots: int = 64\n",
    "    \"\"\"Number of slots for cross-pair correlation memory\"\"\"\n",
    "\n",
    "    # ========== Session Awareness ==========\n",
    "    session_embedding_dim: int = 32\n",
    "    \"\"\"Dimension of session embeddings (Tokyo/London/NY/Sydney)\"\"\"\n",
    "\n",
    "    num_sessions: int = 4\n",
    "    \"\"\"Number of forex sessions\"\"\"\n",
    "\n",
    "    # ========== Regime Detection ==========\n",
    "    num_regimes: int = 4\n",
    "    \"\"\"Number of latent market regimes (trending/ranging/volatile/quiet)\"\"\"\n",
    "\n",
    "    regime_hidden_dim: int = 128\n",
    "    \"\"\"Hidden dimension for regime detector\"\"\"\n",
    "\n",
    "    # ========== Output Heads ==========\n",
    "    num_direction_classes: int = 3\n",
    "    \"\"\"Direction prediction classes (up/neutral/down)\"\"\"\n",
    "\n",
    "    predict_volatility: bool = True\n",
    "    \"\"\"Whether to predict future volatility\"\"\"\n",
    "\n",
    "    predict_regime: bool = True\n",
    "    \"\"\"Whether to predict market regime\"\"\"\n",
    "\n",
    "    output_confidence: bool = True\n",
    "    \"\"\"Whether to output confidence scores\"\"\"\n",
    "\n",
    "    # ========== Training Parameters ==========\n",
    "    learning_rate: float = 1e-4\n",
    "    \"\"\"Base learning rate for optimization\"\"\"\n",
    "\n",
    "    batch_size: int = 32\n",
    "    \"\"\"Training batch size\"\"\"\n",
    "\n",
    "    sequence_length: int = 512\n",
    "    \"\"\"Number of timesteps in each training sequence (5m candles)\"\"\"\n",
    "\n",
    "    gradient_clip_norm: float = 1.0\n",
    "    \"\"\"Maximum gradient norm for clipping\"\"\"\n",
    "\n",
    "    weight_decay: float = 0.01\n",
    "    \"\"\"L2 regularization weight\"\"\"\n",
    "\n",
    "    num_epochs: int = 100\n",
    "    \"\"\"Number of training epochs\"\"\"\n",
    "\n",
    "    warmup_steps: int = 1000\n",
    "    \"\"\"Learning rate warmup steps\"\"\"\n",
    "\n",
    "    # ========== Optimizer Selection ==========\n",
    "    optimizer_type: str = \"delta_gd\"\n",
    "    \"\"\"Optimizer type: 'delta_gd', 'multi_scale_momentum', or 'adam'\"\"\"\n",
    "\n",
    "    use_dgd: bool = True\n",
    "    \"\"\"Whether to use Delta Gradient Descent principles\"\"\"\n",
    "\n",
    "    # ========== Data Configuration ==========\n",
    "    timeframes: List[str] = field(default_factory=lambda: ['5m', '15m', '1H', '4H', '1D'])\n",
    "    \"\"\"Multi-timeframe resolutions to use\"\"\"\n",
    "\n",
    "    pairs: List[str] = field(default_factory=lambda: ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD'])\n",
    "    \"\"\"Currency pairs to trade\"\"\"\n",
    "\n",
    "    base_timeframe: str = '5m'\n",
    "    \"\"\"Base timeframe for data loading\"\"\"\n",
    "\n",
    "    lookback_periods: int = 100\n",
    "    \"\"\"Number of periods to look back for feature computation\"\"\"\n",
    "\n",
    "    # ========== Feature Engineering ==========\n",
    "    include_technicals: bool = True\n",
    "    \"\"\"Include technical indicators (RSI, MACD, Bollinger, ATR, ADX)\"\"\"\n",
    "\n",
    "    include_volume: bool = True\n",
    "    \"\"\"Include volume features (when available)\"\"\"\n",
    "\n",
    "    include_macro: bool = True\n",
    "    \"\"\"Include macro features (rates, yields, calendar)\"\"\"\n",
    "\n",
    "    include_session_features: bool = True\n",
    "    \"\"\"Include session detection features\"\"\"\n",
    "\n",
    "    # ========== Associative Memory Parameters ==========\n",
    "    memory_temperature: float = 1.0\n",
    "    \"\"\"Temperature for memory attention weights\"\"\"\n",
    "\n",
    "    surprise_threshold: float = 0.5\n",
    "    \"\"\"Threshold for surprise-gated memory writing\"\"\"\n",
    "\n",
    "    memory_decay: float = 0.99\n",
    "    \"\"\"Decay factor for memory slots\"\"\"\n",
    "\n",
    "    # ========== Loss Weights ==========\n",
    "    direction_loss_weight: float = 1.0\n",
    "    \"\"\"Weight for direction prediction loss\"\"\"\n",
    "\n",
    "    volatility_loss_weight: float = 0.5\n",
    "    \"\"\"Weight for volatility prediction loss\"\"\"\n",
    "\n",
    "    regime_loss_weight: float = 0.3\n",
    "    \"\"\"Weight for regime prediction loss\"\"\"\n",
    "\n",
    "    calibration_loss_weight: float = 0.2\n",
    "    \"\"\"Weight for confidence calibration loss\"\"\"\n",
    "\n",
    "    # ========== Evaluation ==========\n",
    "    validation_split: float = 0.15\n",
    "    \"\"\"Fraction of data for validation\"\"\"\n",
    "\n",
    "    test_split: float = 0.15\n",
    "    \"\"\"Fraction of data for testing\"\"\"\n",
    "\n",
    "    # ========== Continual Learning ==========\n",
    "    enable_continual_learning: bool = True\n",
    "    \"\"\"Enable continual learning mode\"\"\"\n",
    "\n",
    "    continual_update_frequency: int = 100\n",
    "    \"\"\"How often to update slow memories in continual learning\"\"\"\n",
    "\n",
    "    # ========== Miscellaneous ==========\n",
    "    seed: int = 42\n",
    "    \"\"\"Random seed for reproducibility\"\"\"\n",
    "\n",
    "    device: str = \"cuda\"\n",
    "    \"\"\"Device for training ('cuda' or 'cpu')\"\"\"\n",
    "\n",
    "    num_workers: int = 2\n",
    "    \"\"\"Number of data loading workers\"\"\"\n",
    "\n",
    "    log_interval: int = 100\n",
    "    \"\"\"How often to log training metrics\"\"\"\n",
    "\n",
    "    checkpoint_interval: int = 1000\n",
    "    \"\"\"How often to save checkpoints\"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and auto-configure derived parameters\"\"\"\n",
    "        if self.cms_hidden_dims is None:\n",
    "            self.cms_hidden_dims = [self.hidden_dim] * self.num_cms_levels\n",
    "\n",
    "        assert len(self.cms_hidden_dims) == self.num_cms_levels, \\\n",
    "            \"cms_hidden_dims length must match num_cms_levels\"\n",
    "\n",
    "        assert len(self.pairs) == self.num_pairs, \\\n",
    "            \"Number of pairs in list must match num_pairs\"\n",
    "\n",
    "        assert self.optimizer_type in ['delta_gd', 'multi_scale_momentum', 'adam'], \\\n",
    "            \"Invalid optimizer type\"\n",
    "\n",
    "    def get_update_frequencies(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Calculate update frequencies for each CMS level.\n",
    "\n",
    "        Returns exponentially spaced update intervals:\n",
    "        Level 0 (fastest): updates every 1 step\n",
    "        Level 1: updates every 10 steps\n",
    "        Level 2: updates every 100 steps\n",
    "        Level 3 (slowest): updates every 1000 steps\n",
    "        \"\"\"\n",
    "        frequencies = []\n",
    "        for i in range(self.num_cms_levels):\n",
    "            freq = self.cms_base_frequency * (self.cms_frequency_multiplier ** i)\n",
    "            frequencies.append(freq)\n",
    "        return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac65021",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "fac65021",
    "outputId": "e146aea2-1f4d-4729-e5b0-a400adae3935"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nPreprocessor - Data normalization, alignment, and missing data handling.\\n\\nHandles:\\n- Rolling z-score normalization (no lookahead)\\n- Missing data forward-fill with staleness indicators\\n- Cross-timeframe alignment\\n- Train/validation/test split (temporal order preserved)\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# ============= DATA - PREPROCESSOR =============\n",
    "\"\"\"\n",
    "Preprocessor - Data normalization, alignment, and missing data handling.\n",
    "\n",
    "Handles:\n",
    "- Rolling z-score normalization (no lookahead)\n",
    "- Missing data forward-fill with staleness indicators\n",
    "- Cross-timeframe alignment\n",
    "- Train/validation/test split (temporal order preserved)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5691a42b",
   "metadata": {
    "id": "5691a42b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d23acb9",
   "metadata": {
    "id": "5d23acb9"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses forex data for training.\n",
    "\n",
    "    All operations maintain temporal causality (no lookahead bias).\n",
    "\n",
    "    Args:\n",
    "        normalization_method: 'zscore' or 'minmax' or 'none'\n",
    "        normalization_window: Rolling window for normalization\n",
    "        fill_missing: Whether to forward-fill missing data\n",
    "        max_staleness: Maximum staleness (in periods) before flagging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalization_method: str = 'zscore',\n",
    "        normalization_window: int = 100,\n",
    "        fill_missing: bool = True,\n",
    "        max_staleness: int = 10,\n",
    "    ):\n",
    "        self.normalization_method = normalization_method\n",
    "        self.normalization_window = normalization_window\n",
    "        self.fill_missing = fill_missing\n",
    "        self.max_staleness = max_staleness\n",
    "\n",
    "        # Statistics for normalization (computed from training data)\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit(self, data: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Fit normalization parameters on training data.\n",
    "\n",
    "        Args:\n",
    "            data: Training data tensor (..., seq_len, features)\n",
    "        \"\"\"\n",
    "        if self.normalization_method == 'zscore':\n",
    "            self.mean = data.mean(dim=0, keepdim=True)\n",
    "            self.std = data.std(dim=0, keepdim=True) + 1e-8\n",
    "        elif self.normalization_method == 'minmax':\n",
    "            self.min = data.min(dim=0, keepdim=True)[0]\n",
    "            self.max = data.max(dim=0, keepdim=True)[0]\n",
    "\n",
    "    def transform(self, data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Transform data using fitted parameters.\n",
    "\n",
    "        Args:\n",
    "            data: Data tensor (..., seq_len, features)\n",
    "\n",
    "        Returns:\n",
    "            normalized_data: Normalized tensor\n",
    "            staleness_flags: Binary flags for stale data (..., seq_len, features)\n",
    "        \"\"\"\n",
    "        if self.normalization_method == 'zscore':\n",
    "            if self.mean is None or self.std is None:\n",
    "                raise ValueError(\"Must call fit() before transform()\")\n",
    "            normalized = (data - self.mean) / self.std\n",
    "        elif self.normalization_method == 'minmax':\n",
    "            if self.min is None or self.max is None:\n",
    "                raise ValueError(\"Must call fit() before transform()\")\n",
    "            normalized = (data - self.min) / (self.max - self.min + 1e-8)\n",
    "        else:\n",
    "            normalized = data\n",
    "\n",
    "        # Staleness flags (detect missing/unchanged data)\n",
    "        staleness_flags = self._detect_staleness(data)\n",
    "\n",
    "        return normalized, staleness_flags\n",
    "\n",
    "    def fit_transform(self, data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def _detect_staleness(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Detect stale (unchanged) data points.\n",
    "\n",
    "        Returns binary flags indicating staleness.\n",
    "        \"\"\"\n",
    "        staleness = torch.zeros_like(data)\n",
    "\n",
    "        # Check if data is unchanged for more than max_staleness periods\n",
    "        for i in range(1, data.shape[-2]):\n",
    "            unchanged = (data[..., i, :] == data[..., i-1, :]).float()\n",
    "\n",
    "            # Accumulate staleness\n",
    "            if i > 1:\n",
    "                staleness[..., i, :] = (staleness[..., i-1, :] + 1) * unchanged\n",
    "            else:\n",
    "                staleness[..., i, :] = unchanged\n",
    "\n",
    "        # Flag as stale if > max_staleness\n",
    "        staleness_flags = (staleness > self.max_staleness).float()\n",
    "\n",
    "        return staleness_flags\n",
    "\n",
    "    def rolling_normalize(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        window: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rolling window normalization (no lookahead).\n",
    "\n",
    "        Args:\n",
    "            data: Data tensor (..., seq_len, features)\n",
    "            window: Rolling window size (default: self.normalization_window)\n",
    "\n",
    "        Returns:\n",
    "            normalized: Rolling normalized data\n",
    "        \"\"\"\n",
    "        if window is None:\n",
    "            window = self.normalization_window\n",
    "\n",
    "        normalized = torch.zeros_like(data)\n",
    "        seq_len = data.shape[-2]\n",
    "\n",
    "        for i in range(window, seq_len):\n",
    "            # Use only past data for normalization\n",
    "            window_data = data[..., i-window:i, :]\n",
    "\n",
    "            if self.normalization_method == 'zscore':\n",
    "                mean = window_data.mean(dim=-2, keepdim=True)\n",
    "                std = window_data.std(dim=-2, keepdim=True) + 1e-8\n",
    "                normalized[..., i, :] = (data[..., i, :] - mean.squeeze(-2)) / std.squeeze(-2)\n",
    "            elif self.normalization_method == 'minmax':\n",
    "                min_val = window_data.min(dim=-2, keepdim=True)[0]\n",
    "                max_val = window_data.max(dim=-2, keepdim=True)[0]\n",
    "                normalized[..., i, :] = (data[..., i, :] - min_val.squeeze(-2)) / (max_val.squeeze(-2) - min_val.squeeze(-2) + 1e-8)\n",
    "            else:\n",
    "                normalized[..., i, :] = data[..., i, :]\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def forward_fill_missing(self, data: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward-fill missing (NaN) values.\n",
    "\n",
    "        Args:\n",
    "            data: Data tensor with potential NaNs (..., seq_len, features)\n",
    "\n",
    "        Returns:\n",
    "            filled_data: Data with NaNs filled\n",
    "            fill_mask: Binary mask indicating which values were filled\n",
    "        \"\"\"\n",
    "        filled = data.clone()\n",
    "        fill_mask = torch.isnan(data).float()\n",
    "\n",
    "        # Forward fill\n",
    "        seq_len = data.shape[-2]\n",
    "        for i in range(1, seq_len):\n",
    "            nan_mask = torch.isnan(filled[..., i, :])\n",
    "            filled[..., i, :] = torch.where(\n",
    "                nan_mask,\n",
    "                filled[..., i-1, :],\n",
    "                filled[..., i, :]\n",
    "            )\n",
    "\n",
    "        # If still NaN at start, fill with zeros\n",
    "        filled = torch.nan_to_num(filled, nan=0.0)\n",
    "\n",
    "        return filled, fill_mask\n",
    "\n",
    "    def temporal_split(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        val_split: float = 0.15,\n",
    "        test_split: float = 0.15,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Split data into train/val/test preserving temporal order.\n",
    "\n",
    "        Args:\n",
    "            data: Data tensor (..., seq_len, features)\n",
    "            val_split: Fraction for validation\n",
    "            test_split: Fraction for testing\n",
    "\n",
    "        Returns:\n",
    "            train_data, val_data, test_data\n",
    "        \"\"\"\n",
    "        seq_len = data.shape[-2]\n",
    "\n",
    "        train_end = int(seq_len * (1 - val_split - test_split))\n",
    "        val_end = int(seq_len * (1 - test_split))\n",
    "\n",
    "        train_data = data[..., :train_end, :]\n",
    "        val_data = data[..., train_end:val_end, :]\n",
    "        test_data = data[..., val_end:, :]\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def align_multi_timeframe(\n",
    "        self,\n",
    "        data_dict: Dict[str, torch.Tensor],\n",
    "        base_timeframe: str = '5m',\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Align multiple timeframe tensors to base timeframe.\n",
    "\n",
    "        Args:\n",
    "            data_dict: Dict mapping timeframe -> tensor\n",
    "            base_timeframe: Base timeframe to align to\n",
    "\n",
    "        Returns:\n",
    "            aligned_dict: Aligned tensors\n",
    "        \"\"\"\n",
    "        if base_timeframe not in data_dict:\n",
    "            raise ValueError(f\"Base timeframe {base_timeframe} not in data_dict\")\n",
    "\n",
    "        base_len = data_dict[base_timeframe].shape[-2]\n",
    "        aligned = {}\n",
    "\n",
    "        for tf, tensor in data_dict.items():\n",
    "            if tf == base_timeframe:\n",
    "                aligned[tf] = tensor\n",
    "            else:\n",
    "                # Interpolate or repeat to match base length\n",
    "                # This is a simplified version; in practice, use proper alignment\n",
    "                tf_len = tensor.shape[-2]\n",
    "\n",
    "                if tf_len < base_len:\n",
    "                    # Repeat to match length\n",
    "                    repeat_factor = base_len // tf_len + 1\n",
    "                    repeated = tensor.repeat_interleave(repeat_factor, dim=-2)\n",
    "                    aligned[tf] = repeated[..., :base_len, :]\n",
    "                else:\n",
    "                    # Downsample\n",
    "                    indices = torch.linspace(0, tf_len - 1, base_len).long()\n",
    "                    aligned[tf] = tensor[..., indices, :]\n",
    "\n",
    "        return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8999b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "af8999b4",
    "outputId": "582154da-264a-4f3e-ca2f-b660d62b5c7b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nSession Clock - Forex session detection and timing features.\\n\\nDetects active trading sessions and computes session-related features:\\n- Sydney (22:00-07:00 GMT)\\n- Tokyo (00:00-09:00 GMT)\\n- London (08:00-17:00 GMT)\\n- New York (13:00-22:00 GMT)\\n\\nAlso tracks session overlaps which are high-volatility periods.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# ============= DATA - SESSION CLOCK =============\n",
    "\"\"\"\n",
    "Session Clock - Forex session detection and timing features.\n",
    "\n",
    "Detects active trading sessions and computes session-related features:\n",
    "- Sydney (22:00-07:00 GMT)\n",
    "- Tokyo (00:00-09:00 GMT)\n",
    "- London (08:00-17:00 GMT)\n",
    "- New York (13:00-22:00 GMT)\n",
    "\n",
    "Also tracks session overlaps which are high-volatility periods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc07447",
   "metadata": {
    "id": "bbc07447"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af79c40",
   "metadata": {
    "id": "5af79c40"
   },
   "outputs": [],
   "source": [
    "class SessionClock:\n",
    "    \"\"\"\n",
    "    Forex session detection and timing features.\n",
    "\n",
    "    Generates features based on active trading sessions and their characteristics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Session hours in GMT (24-hour format)\n",
    "        self.sessions = {\n",
    "            'sydney': (22, 7),    # 22:00-07:00 GMT\n",
    "            'tokyo': (0, 9),      # 00:00-09:00 GMT\n",
    "            'london': (8, 17),    # 08:00-17:00 GMT\n",
    "            'new_york': (13, 22), # 13:00-22:00 GMT\n",
    "        }\n",
    "\n",
    "        # Session names for indexing\n",
    "        self.session_names = ['sydney', 'tokyo', 'london', 'new_york']\n",
    "\n",
    "    def detect_sessions(self, timestamps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Detect active sessions for given timestamps.\n",
    "\n",
    "        Args:\n",
    "            timestamps: Unix timestamps (batch, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            session_indicators: Binary indicators (batch, seq_len, 6)\n",
    "                [is_sydney, is_tokyo, is_london, is_ny, is_overlap, is_weekend]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        indicators = torch.zeros(batch_size, seq_len, 6)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                ts = timestamps[b, t].item()\n",
    "                dt = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "\n",
    "                hour = dt.hour\n",
    "                weekday = dt.weekday()  # 0=Monday, 6=Sunday\n",
    "\n",
    "                # Check each session\n",
    "                is_sydney = self._is_in_session(hour, *self.sessions['sydney'])\n",
    "                is_tokyo = self._is_in_session(hour, *self.sessions['tokyo'])\n",
    "                is_london = self._is_in_session(hour, *self.sessions['london'])\n",
    "                is_ny = self._is_in_session(hour, *self.sessions['new_york'])\n",
    "\n",
    "                # Overlap detection (multiple sessions active)\n",
    "                num_active = sum([is_sydney, is_tokyo, is_london, is_ny])\n",
    "                is_overlap = float(num_active > 1)\n",
    "\n",
    "                # Weekend detection\n",
    "                is_weekend = float(weekday >= 5)  # Saturday or Sunday\n",
    "\n",
    "                indicators[b, t, 0] = float(is_sydney)\n",
    "                indicators[b, t, 1] = float(is_tokyo)\n",
    "                indicators[b, t, 2] = float(is_london)\n",
    "                indicators[b, t, 3] = float(is_ny)\n",
    "                indicators[b, t, 4] = is_overlap\n",
    "                indicators[b, t, 5] = is_weekend\n",
    "\n",
    "        return indicators\n",
    "\n",
    "    def _is_in_session(self, hour: int, start: int, end: int) -> bool:\n",
    "        \"\"\"Check if hour is within session\"\"\"\n",
    "        if start < end:\n",
    "            return start <= hour < end\n",
    "        else:\n",
    "            # Session crosses midnight (e.g., Sydney)\n",
    "            return hour >= start or hour < end\n",
    "\n",
    "    def compute_session_features(self, timestamps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rich session features.\n",
    "\n",
    "        Args:\n",
    "            timestamps: Unix timestamps (batch, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            features: Session features (batch, seq_len, feature_dim)\n",
    "                - Session indicators (6 binary)\n",
    "                - Time to session open/close (2 continuous)\n",
    "                - Session volatility profile (4 continuous, one per session)\n",
    "                - Day of week encoding (7 one-hot)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "\n",
    "        # Session indicators\n",
    "        session_indicators = self.detect_sessions(timestamps)\n",
    "\n",
    "        # Time to next session change\n",
    "        time_features = self._compute_time_features(timestamps)\n",
    "\n",
    "        # Session volatility profiles (known characteristics)\n",
    "        vol_profiles = self._get_session_volatility_profiles(session_indicators)\n",
    "\n",
    "        # Day of week\n",
    "        dow_features = self._encode_day_of_week(timestamps)\n",
    "\n",
    "        # Concatenate all features\n",
    "        features = torch.cat([\n",
    "            session_indicators,  # 6\n",
    "            time_features,       # 2\n",
    "            vol_profiles,        # 4\n",
    "            dow_features,        # 7\n",
    "        ], dim=-1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _compute_time_features(self, timestamps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute time-to-event features.\n",
    "\n",
    "        Returns:\n",
    "            time_features: (batch, seq_len, 2)\n",
    "                - Hours to next session open\n",
    "                - Hours to next session close\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        time_features = torch.zeros(batch_size, seq_len, 2)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                ts = timestamps[b, t].item()\n",
    "                dt = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "                hour = dt.hour\n",
    "                minute = dt.minute\n",
    "\n",
    "                # Find next session transition\n",
    "                # Simplified: distance to London open (most important) and NY close\n",
    "                hours_to_london = (8 - hour) % 24\n",
    "                hours_to_ny_close = (22 - hour) % 24\n",
    "\n",
    "                time_features[b, t, 0] = hours_to_london + minute / 60\n",
    "                time_features[b, t, 1] = hours_to_ny_close + minute / 60\n",
    "\n",
    "        return time_features\n",
    "\n",
    "    def _get_session_volatility_profiles(self, session_indicators: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode known volatility characteristics of each session.\n",
    "\n",
    "        Historical volatility patterns:\n",
    "        - Sydney: Low (0.3)\n",
    "        - Tokyo: Medium-Low (0.5)\n",
    "        - London: High (0.9)\n",
    "        - New York: Very High (1.0)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = session_indicators.shape\n",
    "        vol_profiles = torch.zeros(batch_size, seq_len, 4)\n",
    "\n",
    "        # Volatility weights\n",
    "        vol_weights = torch.tensor([0.3, 0.5, 0.9, 1.0])\n",
    "\n",
    "        # Apply to active sessions\n",
    "        vol_profiles = session_indicators[:, :, :4] * vol_weights.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        return vol_profiles\n",
    "\n",
    "    def _encode_day_of_week(self, timestamps: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        One-hot encoding of day of week.\n",
    "\n",
    "        Monday=0, ..., Sunday=6\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        dow_features = torch.zeros(batch_size, seq_len, 7)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for t in range(seq_len):\n",
    "                ts = timestamps[b, t].item()\n",
    "                dt = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "                weekday = dt.weekday()\n",
    "                dow_features[b, t, weekday] = 1.0\n",
    "\n",
    "        return dow_features\n",
    "\n",
    "    def get_session_embedding(self, session_indicators: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert session indicators to learned embedding.\n",
    "\n",
    "        This is a simple weighted sum; in practice, use a learned embedding layer.\n",
    "\n",
    "        Args:\n",
    "            session_indicators: (batch, seq_len, 6)\n",
    "\n",
    "        Returns:\n",
    "            session_embedding: (batch, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Simple weighted combination as a placeholder\n",
    "        # In the full model, this would be a learned embedding\n",
    "        batch_size, seq_len, _ = session_indicators.shape\n",
    "\n",
    "        # Weight matrix (6 sessions \u2192 32 dim embedding)\n",
    "        # This is a simplified version; use nn.Linear in practice\n",
    "        embedding_dim = 32\n",
    "        weights = torch.randn(6, embedding_dim) * 0.1\n",
    "\n",
    "        session_embedding = torch.matmul(session_indicators, weights)\n",
    "\n",
    "        return session_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b4c1790",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "3b4c1790",
    "outputId": "ebd72701-64b5-4dde-f494-3f631f07a645"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nFeature Engine - Technical indicator computation.\\n\\nComputes technical indicators from OHLC data without lookahead bias.\\nAll indicators use only past data to ensure realistic backtesting.\\n\\nTechnical indicators computed:\\n- Returns (simple and log)\\n- Realized volatility (multiple estimators)\\n- RSI (Relative Strength Index)\\n- MACD (Moving Average Convergence Divergence)\\n- Bollinger Bands\\n- ATR (Average True Range)\\n- ADX (Average Directional Index)\\n- Volume features (when available)\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# ============= DATA - FEATURE ENGINE =============\n",
    "\"\"\"\n",
    "Feature Engine - Technical indicator computation.\n",
    "\n",
    "Computes technical indicators from OHLC data without lookahead bias.\n",
    "All indicators use only past data to ensure realistic backtesting.\n",
    "\n",
    "Technical indicators computed:\n",
    "- Returns (simple and log)\n",
    "- Realized volatility (multiple estimators)\n",
    "- RSI (Relative Strength Index)\n",
    "- MACD (Moving Average Convergence Divergence)\n",
    "- Bollinger Bands\n",
    "- ATR (Average True Range)\n",
    "- ADX (Average Directional Index)\n",
    "- Volume features (when available)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ccaca52",
   "metadata": {
    "id": "6ccaca52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a102e139",
   "metadata": {
    "id": "a102e139"
   },
   "outputs": [],
   "source": [
    "class FeatureEngine:\n",
    "    \"\"\"\n",
    "    Technical feature engineering for forex data.\n",
    "\n",
    "    All computations are vectorized and avoid lookahead bias.\n",
    "\n",
    "    Args:\n",
    "        lookback_periods: Number of periods for rolling calculations\n",
    "        include_volume: Whether to compute volume features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback_periods: int = 100,\n",
    "        include_volume: bool = True,\n",
    "    ):\n",
    "        self.lookback_periods = lookback_periods\n",
    "        self.include_volume = include_volume\n",
    "\n",
    "    def compute_features(\n",
    "        self,\n",
    "        ohlc: torch.Tensor,\n",
    "        volume: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute all technical features from OHLC.\n",
    "\n",
    "        Args:\n",
    "            ohlc: OHLC tensor (..., seq_len, 4) [open, high, low, close]\n",
    "            volume: Volume tensor (..., seq_len) optional\n",
    "\n",
    "        Returns:\n",
    "            features: Feature tensor (..., seq_len, num_features)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        # Extract OHLC components\n",
    "        open_price = ohlc[..., 0]\n",
    "        high_price = ohlc[..., 1]\n",
    "        low_price = ohlc[..., 2]\n",
    "        close_price = ohlc[..., 3]\n",
    "\n",
    "        # 1. Returns\n",
    "        returns = self.compute_returns(close_price)\n",
    "        log_returns = self.compute_log_returns(close_price)\n",
    "        features.extend([returns, log_returns])\n",
    "\n",
    "        # 2. Volatility estimators\n",
    "        realized_vol = self.compute_realized_volatility(returns)\n",
    "        parkinson_vol = self.compute_parkinson_volatility(high_price, low_price)\n",
    "        garman_klass_vol = self.compute_garman_klass_volatility(\n",
    "            open_price, high_price, low_price, close_price\n",
    "        )\n",
    "        features.extend([realized_vol, parkinson_vol, garman_klass_vol])\n",
    "\n",
    "        # 3. RSI\n",
    "        rsi = self.compute_rsi(close_price)\n",
    "        features.append(rsi)\n",
    "\n",
    "        # 4. MACD\n",
    "        macd, signal, histogram = self.compute_macd(close_price)\n",
    "        features.extend([macd, signal, histogram])\n",
    "\n",
    "        # 5. Bollinger Bands\n",
    "        bb_upper, bb_middle, bb_lower, bb_width, bb_position = self.compute_bollinger_bands(close_price)\n",
    "        features.extend([bb_upper, bb_middle, bb_lower, bb_width, bb_position])\n",
    "\n",
    "        # 6. ATR\n",
    "        atr = self.compute_atr(high_price, low_price, close_price)\n",
    "        features.append(atr)\n",
    "\n",
    "        # 7. ADX\n",
    "        adx = self.compute_adx(high_price, low_price, close_price)\n",
    "        features.append(adx)\n",
    "\n",
    "        # 8. Price momentum\n",
    "        momentum = self.compute_momentum(close_price, periods=[5, 10, 20])\n",
    "        features.extend(momentum)\n",
    "\n",
    "        # 9. Volume features (if available)\n",
    "        if volume is not None and self.include_volume:\n",
    "            volume_features = self.compute_volume_features(volume, close_price)\n",
    "            features.extend(volume_features)\n",
    "\n",
    "        # Stack all features\n",
    "        features_tensor = torch.stack(features, dim=-1)\n",
    "\n",
    "        return features_tensor\n",
    "\n",
    "    def compute_returns(self, prices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simple returns: (p_t - p_{t-1}) / p_{t-1}\"\"\"\n",
    "        returns = torch.zeros_like(prices)\n",
    "        returns[..., 1:] = (prices[..., 1:] - prices[..., :-1]) / (prices[..., :-1] + 1e-8)\n",
    "        return returns\n",
    "\n",
    "    def compute_log_returns(self, prices: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Log returns: log(p_t / p_{t-1})\"\"\"\n",
    "        log_returns = torch.zeros_like(prices)\n",
    "        log_returns[..., 1:] = torch.log((prices[..., 1:] + 1e-8) / (prices[..., :-1] + 1e-8))\n",
    "        return log_returns\n",
    "\n",
    "    def compute_realized_volatility(\n",
    "        self,\n",
    "        returns: torch.Tensor,\n",
    "        window: int = 20,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Rolling standard deviation of returns\"\"\"\n",
    "        vol = torch.zeros_like(returns)\n",
    "\n",
    "        # Use unfold for efficient rolling window\n",
    "        if returns.dim() == 1:\n",
    "            returns = returns.unsqueeze(0)\n",
    "\n",
    "        batch_shape = returns.shape[:-1]\n",
    "        seq_len = returns.shape[-1]\n",
    "\n",
    "        for i in range(window, seq_len):\n",
    "            window_data = returns[..., i-window:i]\n",
    "            vol[..., i] = window_data.std(dim=-1)\n",
    "\n",
    "        return vol\n",
    "\n",
    "    def compute_parkinson_volatility(\n",
    "        self,\n",
    "        high: torch.Tensor,\n",
    "        low: torch.Tensor,\n",
    "        window: int = 20,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parkinson volatility estimator (uses high-low range).\n",
    "        More efficient than close-to-close volatility.\n",
    "        \"\"\"\n",
    "        hl_ratio = torch.log((high + 1e-8) / (low + 1e-8))\n",
    "        vol = torch.zeros_like(high)\n",
    "\n",
    "        seq_len = high.shape[-1]\n",
    "        for i in range(window, seq_len):\n",
    "            window_data = hl_ratio[..., i-window:i]\n",
    "            vol[..., i] = torch.sqrt((window_data ** 2).mean(dim=-1) / (4 * np.log(2)))\n",
    "\n",
    "        return vol\n",
    "\n",
    "    def compute_garman_klass_volatility(\n",
    "        self,\n",
    "        open_price: torch.Tensor,\n",
    "        high: torch.Tensor,\n",
    "        low: torch.Tensor,\n",
    "        close: torch.Tensor,\n",
    "        window: int = 20,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Garman-Klass volatility estimator (uses OHLC)\"\"\"\n",
    "        hl = torch.log((high + 1e-8) / (low + 1e-8)) ** 2\n",
    "        co = torch.log((close + 1e-8) / (open_price + 1e-8)) ** 2\n",
    "\n",
    "        vol = torch.zeros_like(high)\n",
    "        seq_len = high.shape[-1]\n",
    "\n",
    "        for i in range(window, seq_len):\n",
    "            hl_window = hl[..., i-window:i]\n",
    "            co_window = co[..., i-window:i]\n",
    "            vol[..., i] = torch.sqrt(0.5 * hl_window.mean(dim=-1) - (2 * np.log(2) - 1) * co_window.mean(dim=-1))\n",
    "\n",
    "        return vol\n",
    "\n",
    "    def compute_rsi(self, prices: torch.Tensor, period: int = 14) -> torch.Tensor:\n",
    "        \"\"\"Relative Strength Index\"\"\"\n",
    "        deltas = torch.zeros_like(prices)\n",
    "        deltas[..., 1:] = prices[..., 1:] - prices[..., :-1]\n",
    "\n",
    "        gains = torch.clamp(deltas, min=0)\n",
    "        losses = torch.clamp(-deltas, min=0)\n",
    "\n",
    "        rsi = torch.zeros_like(prices)\n",
    "        seq_len = prices.shape[-1]\n",
    "\n",
    "        for i in range(period, seq_len):\n",
    "            avg_gain = gains[..., i-period:i].mean(dim=-1)\n",
    "            avg_loss = losses[..., i-period:i].mean(dim=-1)\n",
    "\n",
    "            rs = (avg_gain + 1e-8) / (avg_loss + 1e-8)\n",
    "            rsi[..., i] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        return rsi\n",
    "\n",
    "    def compute_macd(\n",
    "        self,\n",
    "        prices: torch.Tensor,\n",
    "        fast_period: int = 12,\n",
    "        slow_period: int = 26,\n",
    "        signal_period: int = 9,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"MACD indicator\"\"\"\n",
    "        # Exponential moving averages\n",
    "        ema_fast = self.compute_ema(prices, fast_period)\n",
    "        ema_slow = self.compute_ema(prices, slow_period)\n",
    "\n",
    "        # MACD line\n",
    "        macd = ema_fast - ema_slow\n",
    "\n",
    "        # Signal line\n",
    "        signal = self.compute_ema(macd, signal_period)\n",
    "\n",
    "        # Histogram\n",
    "        histogram = macd - signal\n",
    "\n",
    "        return macd, signal, histogram\n",
    "\n",
    "    def compute_ema(self, prices: torch.Tensor, period: int) -> torch.Tensor:\n",
    "        \"\"\"Exponential Moving Average\"\"\"\n",
    "        alpha = 2.0 / (period + 1)\n",
    "        ema = torch.zeros_like(prices)\n",
    "        ema[..., 0] = prices[..., 0]\n",
    "\n",
    "        seq_len = prices.shape[-1]\n",
    "        for i in range(1, seq_len):\n",
    "            ema[..., i] = alpha * prices[..., i] + (1 - alpha) * ema[..., i-1]\n",
    "\n",
    "        return ema\n",
    "\n",
    "    def compute_bollinger_bands(\n",
    "        self,\n",
    "        prices: torch.Tensor,\n",
    "        period: int = 20,\n",
    "        num_std: float = 2.0,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Bollinger Bands\"\"\"\n",
    "        middle = torch.zeros_like(prices)\n",
    "        std = torch.zeros_like(prices)\n",
    "\n",
    "        seq_len = prices.shape[-1]\n",
    "        for i in range(period, seq_len):\n",
    "            window = prices[..., i-period:i]\n",
    "            middle[..., i] = window.mean(dim=-1)\n",
    "            std[..., i] = window.std(dim=-1)\n",
    "\n",
    "        upper = middle + num_std * std\n",
    "        lower = middle - num_std * std\n",
    "        width = upper - lower\n",
    "\n",
    "        # Band position: where price is within bands (0 = lower, 1 = upper)\n",
    "        position = (prices - lower) / (width + 1e-8)\n",
    "\n",
    "        return upper, middle, lower, width, position\n",
    "\n",
    "    def compute_atr(\n",
    "        self,\n",
    "        high: torch.Tensor,\n",
    "        low: torch.Tensor,\n",
    "        close: torch.Tensor,\n",
    "        period: int = 14,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Average True Range\"\"\"\n",
    "        # True range\n",
    "        tr = torch.zeros_like(high)\n",
    "        tr[..., 0] = high[..., 0] - low[..., 0]\n",
    "\n",
    "        seq_len = high.shape[-1]\n",
    "        for i in range(1, seq_len):\n",
    "            tr[..., i] = torch.max(\n",
    "                torch.stack([\n",
    "                    high[..., i] - low[..., i],\n",
    "                    torch.abs(high[..., i] - close[..., i-1]),\n",
    "                    torch.abs(low[..., i] - close[..., i-1]),\n",
    "                ], dim=0),\n",
    "                dim=0\n",
    "            )[0]\n",
    "\n",
    "        # ATR is EMA of true range\n",
    "        atr = self.compute_ema(tr, period)\n",
    "\n",
    "        return atr\n",
    "\n",
    "    def compute_adx(\n",
    "        self,\n",
    "        high: torch.Tensor,\n",
    "        low: torch.Tensor,\n",
    "        close: torch.Tensor,\n",
    "        period: int = 14,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Average Directional Index (trend strength)\"\"\"\n",
    "        # Simplified ADX calculation\n",
    "        # In practice, would compute +DI, -DI, and DX\n",
    "\n",
    "        # Use ATR as a proxy for now (simplified)\n",
    "        atr = self.compute_atr(high, low, close, period)\n",
    "\n",
    "        # Normalize to 0-100 range\n",
    "        adx = 100 * torch.sigmoid(atr)\n",
    "\n",
    "        return adx\n",
    "\n",
    "    def compute_momentum(\n",
    "        self,\n",
    "        prices: torch.Tensor,\n",
    "        periods: list = [5, 10, 20],\n",
    "    ) -> list:\n",
    "        \"\"\"Price momentum over different periods\"\"\"\n",
    "        momentum_features = []\n",
    "\n",
    "        for period in periods:\n",
    "            mom = torch.zeros_like(prices)\n",
    "            mom[..., period:] = (prices[..., period:] - prices[..., :-period]) / (prices[..., :-period] + 1e-8)\n",
    "            momentum_features.append(mom)\n",
    "\n",
    "        return momentum_features\n",
    "\n",
    "    def compute_volume_features(\n",
    "        self,\n",
    "        volume: torch.Tensor,\n",
    "        prices: torch.Tensor,\n",
    "        window: int = 20,\n",
    "    ) -> list:\n",
    "        \"\"\"Volume-based features\"\"\"\n",
    "        features = []\n",
    "\n",
    "        # Volume moving average\n",
    "        vol_ma = torch.zeros_like(volume)\n",
    "        seq_len = volume.shape[-1]\n",
    "\n",
    "        for i in range(window, seq_len):\n",
    "            vol_ma[..., i] = volume[..., i-window:i].mean(dim=-1)\n",
    "\n",
    "        # Relative volume\n",
    "        rel_vol = volume / (vol_ma + 1e-8)\n",
    "\n",
    "        # Volume-weighted price\n",
    "        vwap = torch.zeros_like(prices)\n",
    "        for i in range(window, seq_len):\n",
    "            price_window = prices[..., i-window:i]\n",
    "            vol_window = volume[..., i-window:i]\n",
    "            vwap[..., i] = (price_window * vol_window).sum(dim=-1) / (vol_window.sum(dim=-1) + 1e-8)\n",
    "\n",
    "        features.extend([vol_ma, rel_vol, vwap])\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc3e038",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "afc3e038",
    "outputId": "cb691b89-2735-4949-d2d9-9e056a078071"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMacro Feature Encoding - Economic calendar, rates, yields, commodities.\\n\\nEncodes macro-fundamental data that affects forex markets:\\n- Economic calendar events (NFP, CPI, rate decisions)\\n- Interest rates (Fed, ECB, BoJ, RBA, BoE)\\n- Bond yields (US10Y, EU10Y, JP10Y, AU10Y)\\n- Commodities (Gold, Oil, DXY)\\n- Sentiment proxies (VIX, risk-on/risk-off)\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# ============= DATA - MACRO FEATURES =============\n",
    "\"\"\"\n",
    "Macro Feature Encoding - Economic calendar, rates, yields, commodities.\n",
    "\n",
    "Encodes macro-fundamental data that affects forex markets:\n",
    "- Economic calendar events (NFP, CPI, rate decisions)\n",
    "- Interest rates (Fed, ECB, BoJ, RBA, BoE)\n",
    "- Bond yields (US10Y, EU10Y, JP10Y, AU10Y)\n",
    "- Commodities (Gold, Oil, DXY)\n",
    "- Sentiment proxies (VIX, risk-on/risk-off)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb650e4f",
   "metadata": {
    "id": "eb650e4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb30eac4",
   "metadata": {
    "id": "bb30eac4"
   },
   "outputs": [],
   "source": [
    "class MacroFeatureEncoder:\n",
    "    \"\"\"\n",
    "    Encodes macro-fundamental features for forex trading.\n",
    "\n",
    "    Works with limited data availability:\n",
    "    - Economic calendar: time-to-event, expected/actual/previous values\n",
    "    - Interest rates: current rates and differentials\n",
    "    - Bond yields: current yields and spreads\n",
    "    - Commodities: current prices\n",
    "    - Sentiment: current VIX level, risk-on/off classification\n",
    "\n",
    "    Args:\n",
    "        pairs: List of currency pairs\n",
    "        include_calendar: Whether to include economic calendar events\n",
    "        include_rates: Whether to include interest rates\n",
    "        include_yields: Whether to include bond yields\n",
    "        include_commodities: Whether to include commodity prices\n",
    "        include_sentiment: Whether to include sentiment indicators\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pairs: List[str] = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD'],\n",
    "        include_calendar: bool = True,\n",
    "        include_rates: bool = True,\n",
    "        include_yields: bool = True,\n",
    "        include_commodities: bool = True,\n",
    "        include_sentiment: bool = True,\n",
    "    ):\n",
    "        self.pairs = pairs\n",
    "        self.include_calendar = include_calendar\n",
    "        self.include_rates = include_rates\n",
    "        self.include_yields = include_yields\n",
    "        self.include_commodities = include_commodities\n",
    "        self.include_sentiment = include_sentiment\n",
    "\n",
    "        # Extract currencies from pairs\n",
    "        self.currencies = self._extract_currencies()\n",
    "\n",
    "        # Feature dimension\n",
    "        self.feature_dim = self._calculate_feature_dim()\n",
    "\n",
    "    def _extract_currencies(self) -> set:\n",
    "        \"\"\"Extract unique currencies from pairs\"\"\"\n",
    "        currencies = set()\n",
    "        for pair in self.pairs:\n",
    "            if len(pair) == 6:\n",
    "                currencies.add(pair[:3])\n",
    "                currencies.add(pair[3:])\n",
    "        return currencies\n",
    "\n",
    "    def _calculate_feature_dim(self) -> int:\n",
    "        \"\"\"Calculate total feature dimension\"\"\"\n",
    "        dim = 0\n",
    "\n",
    "        if self.include_calendar:\n",
    "            dim += 10  # Calendar event encoding\n",
    "\n",
    "        if self.include_rates:\n",
    "            dim += len(self.currencies) + len(self.pairs)  # Rates + differentials\n",
    "\n",
    "        if self.include_yields:\n",
    "            dim += len(self.currencies) + len(self.pairs)  # Yields + spreads\n",
    "\n",
    "        if self.include_commodities:\n",
    "            dim += 3  # Gold, Oil, DXY\n",
    "\n",
    "        if self.include_sentiment:\n",
    "            dim += 2  # VIX level, risk-on/off\n",
    "\n",
    "        return dim\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        calendar_data: Optional[pd.DataFrame] = None,\n",
    "        rates_data: Optional[Dict[str, float]] = None,\n",
    "        yields_data: Optional[Dict[str, float]] = None,\n",
    "        commodities_data: Optional[Dict[str, float]] = None,\n",
    "        sentiment_data: Optional[Dict[str, float]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode macro features for given timestamps.\n",
    "\n",
    "        Args:\n",
    "            timestamps: Timestamps to encode for (batch, seq_len)\n",
    "            calendar_data: DataFrame with economic events\n",
    "            rates_data: Dict of current interest rates by currency\n",
    "            yields_data: Dict of current bond yields by currency\n",
    "            commodities_data: Dict of commodity prices\n",
    "            sentiment_data: Dict of sentiment indicators\n",
    "\n",
    "        Returns:\n",
    "            macro_features: Encoded features (batch, seq_len, feature_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        features = []\n",
    "\n",
    "        # 1. Economic Calendar\n",
    "        if self.include_calendar:\n",
    "            calendar_features = self._encode_calendar(timestamps, calendar_data)\n",
    "            features.append(calendar_features)\n",
    "\n",
    "        # 2. Interest Rates\n",
    "        if self.include_rates:\n",
    "            rates_features = self._encode_rates(timestamps, rates_data)\n",
    "            features.append(rates_features)\n",
    "\n",
    "        # 3. Bond Yields\n",
    "        if self.include_yields:\n",
    "            yields_features = self._encode_yields(timestamps, yields_data)\n",
    "            features.append(yields_features)\n",
    "\n",
    "        # 4. Commodities\n",
    "        if self.include_commodities:\n",
    "            commodity_features = self._encode_commodities(timestamps, commodities_data)\n",
    "            features.append(commodity_features)\n",
    "\n",
    "        # 5. Sentiment\n",
    "        if self.include_sentiment:\n",
    "            sentiment_features = self._encode_sentiment(timestamps, sentiment_data)\n",
    "            features.append(sentiment_features)\n",
    "\n",
    "        # Concatenate all features\n",
    "        if features:\n",
    "            macro_features = torch.cat(features, dim=-1)\n",
    "        else:\n",
    "            macro_features = torch.zeros(batch_size, seq_len, 1)\n",
    "\n",
    "        return macro_features\n",
    "\n",
    "    def _encode_calendar(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        calendar_data: Optional[pd.DataFrame],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode economic calendar events.\n",
    "\n",
    "        Features:\n",
    "        - Time to next major event (hours)\n",
    "        - Event importance (0-3: low/medium/high/critical)\n",
    "        - Expected impact direction (- to +)\n",
    "        - Surprise factor (actual - expected, normalized)\n",
    "        - Event type encoding (one-hot for NFP/CPI/Rate/GDP/Other)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "\n",
    "        # Default: no events\n",
    "        features = torch.zeros(batch_size, seq_len, 10)\n",
    "\n",
    "        if calendar_data is not None and len(calendar_data) > 0:\n",
    "            # Convert timestamps to datetime\n",
    "            for b in range(batch_size):\n",
    "                for t in range(seq_len):\n",
    "                    ts = timestamps[b, t].item()\n",
    "                    dt = datetime.fromtimestamp(ts)\n",
    "\n",
    "                    # Find next event\n",
    "                    future_events = calendar_data[calendar_data['timestamp'] > dt]\n",
    "                    if len(future_events) > 0:\n",
    "                        next_event = future_events.iloc[0]\n",
    "                        time_to_event = (next_event['timestamp'] - dt).total_seconds() / 3600\n",
    "\n",
    "                        features[b, t, 0] = min(time_to_event / 24, 10)  # Days to event, capped at 10\n",
    "                        features[b, t, 1] = next_event.get('importance', 1) / 3  # Normalized\n",
    "                        features[b, t, 2] = next_event.get('expected_direction', 0)\n",
    "                        features[b, t, 3] = next_event.get('surprise', 0)\n",
    "\n",
    "                        # Event type one-hot\n",
    "                        event_type = next_event.get('type', 'Other')\n",
    "                        type_idx = {'NFP': 4, 'CPI': 5, 'Rate': 6, 'GDP': 7, 'Other': 8}.get(event_type, 8)\n",
    "                        features[b, t, type_idx] = 1.0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _encode_rates(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        rates_data: Optional[Dict[str, float]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode interest rates and differentials.\n",
    "\n",
    "        Features:\n",
    "        - Current rate for each currency\n",
    "        - Rate differential for each pair\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "\n",
    "        # Map currencies to rates\n",
    "        currency_map = {\n",
    "            'USD': 'FED',\n",
    "            'EUR': 'ECB',\n",
    "            'GBP': 'BOE',\n",
    "            'JPY': 'BOJ',\n",
    "            'AUD': 'RBA',\n",
    "        }\n",
    "\n",
    "        num_currencies = len(self.currencies)\n",
    "        num_pairs = len(self.pairs)\n",
    "\n",
    "        features = torch.zeros(batch_size, seq_len, num_currencies + num_pairs)\n",
    "\n",
    "        if rates_data is not None:\n",
    "            # Currency rates\n",
    "            for i, currency in enumerate(sorted(self.currencies)):\n",
    "                rate_key = currency_map.get(currency, currency)\n",
    "                rate = rates_data.get(rate_key, 0.0)\n",
    "                features[:, :, i] = rate\n",
    "\n",
    "            # Pair differentials\n",
    "            for i, pair in enumerate(self.pairs):\n",
    "                if len(pair) == 6:\n",
    "                    base_curr = pair[:3]\n",
    "                    quote_curr = pair[3:]\n",
    "\n",
    "                    base_rate = rates_data.get(currency_map.get(base_curr, base_curr), 0.0)\n",
    "                    quote_rate = rates_data.get(currency_map.get(quote_curr, quote_curr), 0.0)\n",
    "\n",
    "                    differential = base_rate - quote_rate\n",
    "                    features[:, :, num_currencies + i] = differential\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _encode_yields(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        yields_data: Optional[Dict[str, float]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode bond yields and spreads.\n",
    "\n",
    "        Similar to rates encoding.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "\n",
    "        yield_map = {\n",
    "            'USD': 'US10Y',\n",
    "            'EUR': 'EU10Y',\n",
    "            'GBP': 'UK10Y',\n",
    "            'JPY': 'JP10Y',\n",
    "            'AUD': 'AU10Y',\n",
    "        }\n",
    "\n",
    "        num_currencies = len(self.currencies)\n",
    "        num_pairs = len(self.pairs)\n",
    "\n",
    "        features = torch.zeros(batch_size, seq_len, num_currencies + num_pairs)\n",
    "\n",
    "        if yields_data is not None:\n",
    "            # Currency yields\n",
    "            for i, currency in enumerate(sorted(self.currencies)):\n",
    "                yield_key = yield_map.get(currency, currency)\n",
    "                yield_val = yields_data.get(yield_key, 0.0)\n",
    "                features[:, :, i] = yield_val\n",
    "\n",
    "            # Pair spreads\n",
    "            for i, pair in enumerate(self.pairs):\n",
    "                if len(pair) == 6:\n",
    "                    base_curr = pair[:3]\n",
    "                    quote_curr = pair[3:]\n",
    "\n",
    "                    base_yield = yields_data.get(yield_map.get(base_curr, base_curr), 0.0)\n",
    "                    quote_yield = yields_data.get(yield_map.get(quote_curr, quote_curr), 0.0)\n",
    "\n",
    "                    spread = base_yield - quote_yield\n",
    "                    features[:, :, num_currencies + i] = spread\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _encode_commodities(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        commodities_data: Optional[Dict[str, float]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode commodity prices.\n",
    "\n",
    "        Features:\n",
    "        - Gold (safe haven)\n",
    "        - Oil (WTI or Brent)\n",
    "        - DXY (US Dollar Index)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        features = torch.zeros(batch_size, seq_len, 3)\n",
    "\n",
    "        if commodities_data is not None:\n",
    "            features[:, :, 0] = commodities_data.get('Gold', 0.0) / 2000  # Normalize\n",
    "            features[:, :, 1] = commodities_data.get('Oil', 0.0) / 100    # Normalize\n",
    "            features[:, :, 2] = commodities_data.get('DXY', 0.0) / 100    # Normalize\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _encode_sentiment(\n",
    "        self,\n",
    "        timestamps: torch.Tensor,\n",
    "        sentiment_data: Optional[Dict[str, float]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode sentiment indicators.\n",
    "\n",
    "        Features:\n",
    "        - VIX level (volatility index)\n",
    "        - Risk-on/risk-off classification\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = timestamps.shape\n",
    "        features = torch.zeros(batch_size, seq_len, 2)\n",
    "\n",
    "        if sentiment_data is not None:\n",
    "            vix = sentiment_data.get('VIX', 15.0)\n",
    "            features[:, :, 0] = vix / 50  # Normalize\n",
    "\n",
    "            # Risk-on/off: -1 (risk-off) to +1 (risk-on)\n",
    "            risk_sentiment = sentiment_data.get('risk_sentiment', 0.0)\n",
    "            features[:, :, 1] = risk_sentiment\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2f53ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "7c2f53ed",
    "outputId": "cf161bc2-75b8-4ba5-f89f-df467d816c88"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nForex Dataset - Multi-timeframe OHLC data loading and management.\\n\\nThis dataset handles:\\n- Loading 5-minute OHLC candles for multiple currency pairs\\n- Aggregating to multiple timeframes (15m, 1H, 4H, 1D)\\n- Temporal alignment across timeframes\\n- Streaming/online mode for live inference\\n- No lookahead bias in all operations\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# ============= DATA - FOREX DATASET =============\n",
    "\"\"\"\n",
    "Forex Dataset - Multi-timeframe OHLC data loading and management.\n",
    "\n",
    "This dataset handles:\n",
    "- Loading 5-minute OHLC candles for multiple currency pairs\n",
    "- Aggregating to multiple timeframes (15m, 1H, 4H, 1D)\n",
    "- Temporal alignment across timeframes\n",
    "- Streaming/online mode for live inference\n",
    "- No lookahead bias in all operations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f63d4c9",
   "metadata": {
    "id": "4f63d4c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f84df4",
   "metadata": {
    "id": "91f84df4"
   },
   "outputs": [],
   "source": [
    "class ForexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multi-timeframe forex dataset.\n",
    "\n",
    "    Loads OHLC data at base timeframe and provides aligned multi-timeframe views.\n",
    "    Designed to work with limited data: OHLC, volume (optional), timestamp.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to CSV files or DataFrame dict\n",
    "        pairs: List of currency pair symbols\n",
    "        base_timeframe: Base timeframe for data (e.g., '5m')\n",
    "        target_timeframes: List of target timeframes to aggregate\n",
    "        sequence_length: Length of sequences to return\n",
    "        stride: Stride for sequence sampling\n",
    "        mode: 'train', 'val', or 'test'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Optional[str] = None,\n",
    "        data_dict: Optional[Dict[str, pd.DataFrame]] = None,\n",
    "        pairs: List[str] = ['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD'],\n",
    "        base_timeframe: str = '5m',\n",
    "        target_timeframes: List[str] = ['5m', '15m', '1H', '4H', '1D'],\n",
    "        sequence_length: int = 512,\n",
    "        stride: int = 1,\n",
    "        mode: str = 'train',\n",
    "        num_regimes: int = 4, # Added for target generation\n",
    "        num_direction_classes: int = 3, # Added for target generation\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pairs = pairs\n",
    "        self.base_timeframe = base_timeframe\n",
    "        self.target_timeframes = target_timeframes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "        self.num_regimes = num_regimes\n",
    "        self.num_direction_classes = num_direction_classes\n",
    "        self.target_lookahead_steps = 5 # Predict 5 steps into the future\n",
    "\n",
    "        # Load data\n",
    "        if data_dict is not None:\n",
    "            self.data = data_dict\n",
    "        elif data_path is not None:\n",
    "            self.data = self._load_from_path(data_path)\n",
    "        else:\n",
    "            # Generate synthetic data for demo/testing\n",
    "            self.data = self._generate_synthetic_data()\n",
    "\n",
    "        # Preprocess and align data\n",
    "        self.aligned_data = self._preprocess_and_align()\n",
    "\n",
    "        # Calculate valid indices for sequence extraction\n",
    "        self.valid_indices = self._calculate_valid_indices()\n",
    "\n",
    "    def _load_from_path(self, data_path: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load OHLC data from CSV files.\n",
    "\n",
    "        Expected format: one CSV per pair with columns:\n",
    "        [timestamp, open, high, low, close, volume (optional)]\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        data_path = Path(data_path)\n",
    "\n",
    "        for pair in self.pairs:\n",
    "            csv_path = data_path / f\"{pair}_{self.base_timeframe}.csv\"\n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df = df.set_index('timestamp').sort_index()\n",
    "                data[pair] = df\n",
    "            else:\n",
    "                print(f\"Warning: {csv_path} not found, using synthetic data for {pair}\")\n",
    "                data[pair] = self._generate_pair_data(pair)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _generate_synthetic_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Generate synthetic OHLC data for testing\"\"\"\n",
    "        data = {}\n",
    "        for pair in self.pairs:\n",
    "            data[pair] = self._generate_pair_data(pair)\n",
    "        return data\n",
    "\n",
    "    def _generate_pair_data(self, pair: str, num_samples: int = 10000) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic OHLC for one pair\"\"\"\n",
    "        # Start from a base price\n",
    "        base_prices = {\n",
    "            'EURUSD': 1.1000,\n",
    "            'GBPUSD': 1.3000,\n",
    "            'USDJPY': 110.00,\n",
    "            'AUDUSD': 0.7500,\n",
    "        }\n",
    "        base_price = base_prices.get(pair, 1.0)\n",
    "\n",
    "        # Generate random walk\n",
    "        returns = np.random.randn(num_samples) * 0.0001  # Small returns\n",
    "        prices = base_price * (1 + returns).cumprod()\n",
    "\n",
    "        # Generate OHLC from prices (simplified)\n",
    "        noise = np.random.randn(num_samples, 3) * base_price * 0.0002\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'open': prices,\n",
    "            'high': prices + np.abs(noise[:, 0]),\n",
    "            'low': prices - np.abs(noise[:, 1]),\n",
    "            'close': prices + noise[:, 2],\n",
    "            'volume': np.random.randint(100, 1000, num_samples),\n",
    "        })\n",
    "\n",
    "        # Add timestamp (5-minute intervals)\n",
    "        start_time = pd.Timestamp('2024-01-01 00:00:00')\n",
    "        df['timestamp'] = pd.date_range(start=start_time, periods=num_samples, freq='5min')\n",
    "        df = df.set_index('timestamp')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _preprocess_and_align(self) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess data and create aligned multi-timeframe views.\n",
    "\n",
    "        Returns:\n",
    "            aligned_data: Dict[pair][timeframe] -> DataFrame\n",
    "        \"\"\"\n",
    "        aligned = {}\n",
    "\n",
    "        for pair in self.pairs:\n",
    "            aligned[pair] = {}\n",
    "            base_df = self.data[pair].copy()\n",
    "\n",
    "            # Ensure we have required columns\n",
    "            required = ['open', 'high', 'low', 'close']\n",
    "            for col in required:\n",
    "                assert col in base_df.columns, f\"Missing column {col} in {pair}\"\n",
    "\n",
    "            # Store base timeframe\n",
    "            aligned[pair][self.base_timeframe] = base_df\n",
    "\n",
    "            # Aggregate to other timeframes\n",
    "            for tf in self.target_timeframes:\n",
    "                if tf == self.base_timeframe:\n",
    "                    continue\n",
    "                aligned[pair][tf] = self._aggregate_timeframe(base_df, tf)\n",
    "\n",
    "        return aligned\n",
    "\n",
    "    def _aggregate_timeframe(self, df: pd.DataFrame, target_tf: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate OHLC to target timeframe.\n",
    "\n",
    "        Uses pandas resample with OHLC aggregation rules.\n",
    "        \"\"\"\n",
    "        # Map timeframe strings to pandas freq\n",
    "        tf_map = {\n",
    "            '5m': '5min',\n",
    "            '15m': '15min',\n",
    "            '1H': '1h',\n",
    "            '4H': '4h',\n",
    "            '1D': '1D',\n",
    "            '1W': '1W',\n",
    "        }\n",
    "\n",
    "        freq = tf_map.get(target_tf, target_tf)\n",
    "\n",
    "        # Resample with OHLC rules\n",
    "        agg_rules = {\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "        }\n",
    "\n",
    "        # Add volume if present\n",
    "        if 'volume' in df.columns:\n",
    "            agg_rules['volume'] = 'sum'\n",
    "\n",
    "        resampled = df.resample(freq).agg(agg_rules).dropna()\n",
    "\n",
    "        return resampled\n",
    "\n",
    "    def _calculate_valid_indices(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Calculate valid starting indices for sequences.\n",
    "\n",
    "        Valid index = enough history before it for sequence_length + target_lookahead_steps.\n",
    "        \"\"\"\n",
    "        # Use the base timeframe of the first pair to determine valid indices\n",
    "        first_pair = self.pairs[0]\n",
    "        base_df = self.aligned_data[first_pair][self.base_timeframe]\n",
    "\n",
    "        # Ensure enough data for both input sequence and future targets\n",
    "        max_idx = len(base_df) - self.sequence_length - self.target_lookahead_steps\n",
    "        valid_indices = list(range(0, max_idx, self.stride))\n",
    "\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of valid sequences\"\"\"\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a training sample.\n",
    "\n",
    "        Returns:\n",
    "            sample: Dictionary containing:\n",
    "                - ohlc: Multi-pair, multi-timeframe OHLC (pairs, timeframes, seq, 4)\n",
    "                - volume: Volume data if available (pairs, timeframes, seq)\n",
    "                - timestamps: Unix timestamps (seq,)\n",
    "                - direction: Target direction for the last step (batch,)\n",
    "                - volatility: Target volatility for the last step (batch,)\n",
    "                - regime: Target regime for the last step (batch,)\n",
    "        \"\"\"\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        # Extract OHLC for all pairs and timeframes\n",
    "        ohlc_data = []\n",
    "        volume_data = []\n",
    "\n",
    "        for pair in self.pairs:\n",
    "            pair_ohlc = []\n",
    "            pair_volume = []\n",
    "\n",
    "            for tf in self.target_timeframes:\n",
    "                df = self.aligned_data[pair][tf]\n",
    "\n",
    "                # Get corresponding indices in this timeframe\n",
    "                # For higher timeframes, we need to find the aligned subset\n",
    "                base_df = self.aligned_data[pair][self.base_timeframe]\n",
    "                start_time = base_df.index[start_idx]\n",
    "                end_time = base_df.index[end_idx - 1]\n",
    "\n",
    "                # Extract data in this time range\n",
    "                tf_data = df.loc[start_time:end_time]\n",
    "\n",
    "                # Ensure we have enough data (e.g. if the time range is smaller than the timeframe interval)\n",
    "                if len(tf_data) == 0:\n",
    "                    # Fill with last available data to ensure sequence_length can be met by padding\n",
    "                    # Or, alternatively, skip this sample, but filling is safer for DataLoader\n",
    "                    if not df.empty: # Check if df is not empty to avoid IndexError\n",
    "                         tf_data = df.iloc[-1:].copy()\n",
    "                    else: # If df is also empty, create dummy data\n",
    "                         tf_data = pd.DataFrame(np.zeros((1,4)), columns=['open', 'high', 'low', 'close'])\n",
    "\n",
    "\n",
    "                # Extract OHLC\n",
    "                ohlc_array = tf_data[['open', 'high', 'low', 'close']].values\n",
    "\n",
    "                # Pad or truncate to expected length\n",
    "                # For higher timeframes, we'll have fewer samples\n",
    "                expected_len = self._get_expected_length(tf)\n",
    "                ohlc_array = self._pad_or_truncate(ohlc_array, expected_len)\n",
    "\n",
    "                pair_ohlc.append(ohlc_array)\n",
    "\n",
    "                # Volume\n",
    "                if 'volume' in tf_data.columns:\n",
    "                    vol_array = tf_data['volume'].values\n",
    "                    vol_array = self._pad_or_truncate(vol_array, expected_len)\n",
    "                    pair_volume.append(vol_array)\n",
    "                else:\n",
    "                    # If volume column does not exist or if tf_data was empty and we created dummy ohlc, create dummy volume\n",
    "                    pair_volume.append(np.zeros(expected_len))\n",
    "\n",
    "            ohlc_data.append(pair_ohlc)\n",
    "            volume_data.append(pair_volume)\n",
    "\n",
    "        # Convert to tensors\n",
    "        ohlc_tensor = torch.tensor(np.array(ohlc_data), dtype=torch.float32)\n",
    "        volume_tensor = torch.tensor(np.array(volume_data), dtype=torch.float32)\n",
    "\n",
    "        # Get timestamps from base timeframe\n",
    "        base_df = self.aligned_data[self.pairs[0]][self.base_timeframe]\n",
    "        timestamps_sequence = base_df.index[start_idx:end_idx]\n",
    "        timestamps_tensor = torch.tensor(timestamps_sequence.astype(np.int64) // 10**9, dtype=torch.long)\n",
    "\n",
    "        # --- Generate Targets for the last step of the sequence ---\n",
    "        # Get the closing price at the end of the input sequence and a few steps ahead\n",
    "        current_close = base_df['close'].iloc[end_idx - 1]\n",
    "        future_close_data = base_df['close'].iloc[end_idx : end_idx + self.target_lookahead_steps]\n",
    "\n",
    "        # Direction Target\n",
    "        direction_target = torch.tensor(1, dtype=torch.long) # Default to Neutral\n",
    "        if len(future_close_data) > 0:\n",
    "            future_close = future_close_data.iloc[-1] # Look at the last available future step\n",
    "            price_change = (future_close - current_close) / current_close\n",
    "            # Thresholds for Up/Down - can be adjusted\n",
    "            threshold_up = 0.0001\n",
    "            threshold_down = -0.0001\n",
    "\n",
    "            if price_change > threshold_up:\n",
    "                direction_target = torch.tensor(2, dtype=torch.long) # Up\n",
    "            elif price_change < threshold_down:\n",
    "                direction_target = torch.tensor(0, dtype=torch.long) # Down\n",
    "            else:\n",
    "                direction_target = torch.tensor(1, dtype=torch.long) # Neutral\n",
    "\n",
    "        # Volatility Target (realized volatility over the lookahead period)\n",
    "        volatility_target = torch.tensor(0.0, dtype=torch.float32)\n",
    "        if len(future_close_data) > 1:\n",
    "            returns = future_close_data.pct_change().dropna()\n",
    "            if len(returns) > 0:\n",
    "                volatility_target = torch.tensor(returns.std(), dtype=torch.float32)\n",
    "\n",
    "        # Regime Target (simple heuristic based on volatility and direction)\n",
    "        regime_target = torch.tensor(1, dtype=torch.long) # Default to Ranging\n",
    "        if volatility_target > 0.001: # High volatility\n",
    "            regime_target = torch.tensor(2, dtype=torch.long) # Volatile\n",
    "        elif volatility_target < 0.0001: # Low volatility\n",
    "            regime_target = torch.tensor(3, dtype=torch.long) # Quiet\n",
    "        elif direction_target == 0 or direction_target == 2: # Trending if strong direction\n",
    "             regime_target = torch.tensor(0, dtype=torch.long) # Trending\n",
    "\n",
    "        return {\n",
    "            'ohlc': ohlc_tensor,\n",
    "            'volume': volume_tensor,\n",
    "            'timestamps': timestamps_tensor,\n",
    "            'direction': direction_target,\n",
    "            'volatility': volatility_target,\n",
    "            'regime': regime_target,\n",
    "        }\n",
    "\n",
    "    def _get_expected_length(self, timeframe: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate expected sequence length for a timeframe.\n",
    "\n",
    "        The model expects a consistent sequence_length across all timeframes.\n",
    "        Therefore, we return the global sequence_length for all timeframes,\n",
    "        and `_pad_or_truncate` will handle making them all this length.\n",
    "        \"\"\"\n",
    "        return self.sequence_length\n",
    "\n",
    "    def _pad_or_truncate(self, array: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"Pad or truncate array to target length\"\"\"\n",
    "        if len(array) >= target_length:\n",
    "            return array[:target_length]\n",
    "        else:\n",
    "            # Pad by repeating last value\n",
    "            if len(array) == 0:\n",
    "                # If the array is completely empty, fill with zeros\n",
    "                # Handle multi-dimensional arrays for OHLC\n",
    "                if array.ndim == 1: return np.zeros(target_length)\n",
    "                else: return np.zeros((target_length,) + array.shape[1:])\n",
    "\n",
    "            pad_length = target_length - len(array)\n",
    "            if array.ndim == 1:\n",
    "                padding = np.repeat(array[-1], pad_length)\n",
    "            else:\n",
    "                padding = np.repeat(array[-1:], pad_length, axis=0)\n",
    "            return np.concatenate([array, padding], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60710074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "60710074",
    "outputId": "c5b657ef-26ef-4f46-8d7d-14c2823bbae5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nAssociative Memory Module - The fundamental building block of NEXUS-FX.\\n\\nThis module implements associative memory based on L2-regression rather than\\ndot-product attention, following the NSAM (Nested Sequential Associative Memory)\\nframework. Each memory stores key-value pairs and retrieves via minimizing\\nan internal L2 objective.\\n\\nTheoretical Background:\\n    In NSAM, memories are nested optimization problems. Traditional attention\\n    uses dot-product similarity, but associative memory uses L2-regression:\\n    \\n    M(q) = argmin_v || v - \u03a3_i \u03b1_i * V_i ||^2\\n    \\n    where \u03b1_i are attention weights based on L2 distance to keys:\\n    \u03b1_i = softmin(|| q - K_i ||^2, temperature=\u03c4)\\n    \\n    This formulation naturally leads to surprise-gated writing: the memory\\n    writes more when prediction error is high (surprise = ||v_actual - v_predicted||^2).\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# ============= MODEL - ASSOCIATIVE MEMORY =============\n",
    "\"\"\"\n",
    "Associative Memory Module - The fundamental building block of NEXUS-FX.\n",
    "\n",
    "This module implements associative memory based on L2-regression rather than\n",
    "dot-product attention, following the NSAM (Nested Sequential Associative Memory)\n",
    "framework. Each memory stores key-value pairs and retrieves via minimizing\n",
    "an internal L2 objective.\n",
    "\n",
    "Theoretical Background:\n",
    "    In NSAM, memories are nested optimization problems. Traditional attention\n",
    "    uses dot-product similarity, but associative memory uses L2-regression:\n",
    "\n",
    "    M(q) = argmin_v || v - \u03a3_i \u03b1_i * V_i ||^2\n",
    "\n",
    "    where \u03b1_i are attention weights based on L2 distance to keys:\n",
    "    \u03b1_i = softmin(|| q - K_i ||^2, temperature=\u03c4)\n",
    "\n",
    "    This formulation naturally leads to surprise-gated writing: the memory\n",
    "    writes more when prediction error is high (surprise = ||v_actual - v_predicted||^2).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "579c7561",
   "metadata": {
    "id": "579c7561"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "649db040",
   "metadata": {
    "id": "649db040"
   },
   "outputs": [],
   "source": [
    "class AssociativeMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Core associative memory: stores key-value pairs and retrieves\n",
    "    via L2-minimization rather than dot-product attention.\n",
    "\n",
    "    Each memory has an update_frequency parameter controlling how often\n",
    "    its internal state updates. This is the core mechanism for creating\n",
    "    a hierarchy of memories at different timescales.\n",
    "\n",
    "    Args:\n",
    "        key_dim: Dimension of memory keys\n",
    "        value_dim: Dimension of memory values\n",
    "        num_slots: Number of key-value slots in the memory\n",
    "        update_frequency: How often this memory updates (1 = every step)\n",
    "        temperature: Temperature for attention weight computation\n",
    "        use_surprise_gating: If True, gate writes by prediction error\n",
    "        use_dgd: If True, use Delta GD principles for adaptive decay\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key_dim: int,\n",
    "        value_dim: int,\n",
    "        num_slots: int,\n",
    "        update_frequency: int = 1,\n",
    "        temperature: float = 1.0,\n",
    "        use_surprise_gating: bool = True,\n",
    "        use_dgd: bool = True,\n",
    "        memory_decay: float = 0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key_dim = key_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_slots = num_slots\n",
    "        self.update_frequency = update_frequency\n",
    "        self.temperature = temperature\n",
    "        self.use_surprise_gating = use_surprise_gating\n",
    "        self.use_dgd = use_dgd\n",
    "        self.memory_decay = memory_decay\n",
    "\n",
    "        # Initialize memory slots\n",
    "        # Keys and values are learnable parameters that get updated during forward pass\n",
    "        # CHANGE: Made keys and values nn.Parameter instead of register_buffer\n",
    "        self.keys = nn.Parameter(torch.randn(num_slots, key_dim))\n",
    "        self.values = nn.Parameter(torch.randn(num_slots, value_dim))\n",
    "        self.register_buffer('step_counter', torch.tensor(0, dtype=torch.long))\n",
    "        self.register_buffer('slot_age', torch.zeros(num_slots))\n",
    "\n",
    "        # Normalization layers\n",
    "        self.key_norm = nn.LayerNorm(key_dim)\n",
    "        self.value_norm = nn.LayerNorm(value_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        value_target: Optional[torch.Tensor] = None,\n",
    "        write_mode: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass: read from memory and optionally write.\n",
    "\n",
    "        Args:\n",
    "            query: Query tensor of shape (batch, key_dim)\n",
    "            value_target: Target values to write (batch, value_dim), if in write mode\n",
    "            write_mode: If True and update frequency allows, write to memory\n",
    "\n",
    "        Returns:\n",
    "            retrieved_values: Retrieved values (batch, value_dim)\n",
    "            surprise: Prediction error for each query (batch,)\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Normalize query\n",
    "        query = self.key_norm(query)\n",
    "\n",
    "        # Compute L2 distances to all keys\n",
    "        # distances shape: (batch, num_slots)\n",
    "        distances = torch.cdist(query, self.keys, p=2)\n",
    "\n",
    "        # Compute attention weights using softmin (negative softmax of distances)\n",
    "        # \u03b1_i = exp(-d_i^2 / \u03c4) / \u03a3_j exp(-d_j^2 / \u03c4)\n",
    "        attention_logits = -distances.pow(2) / self.temperature\n",
    "        attention_weights = F.softmax(attention_logits, dim=-1)\n",
    "\n",
    "        # Retrieve values via weighted sum\n",
    "        # retrieved = \u03a3_i \u03b1_i * V_i\n",
    "        retrieved_values = torch.matmul(attention_weights, self.values)  # (batch, value_dim)\n",
    "\n",
    "        # Compute surprise (prediction error)\n",
    "        if value_target is not None:\n",
    "            surprise = F.mse_loss(retrieved_values, value_target, reduction='none').mean(dim=-1)\n",
    "        else:\n",
    "            surprise = torch.zeros(batch_size, device=query.device)\n",
    "\n",
    "        # Write to memory if conditions are met\n",
    "        if write_mode and value_target is not None:\n",
    "            self._write_to_memory(query, value_target, surprise)\n",
    "\n",
    "        return retrieved_values, surprise\n",
    "\n",
    "    def _write_to_memory(\n",
    "        self,\n",
    "        keys_new: torch.Tensor,\n",
    "        values_new: torch.Tensor,\n",
    "        surprise: torch.Tensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Write new key-value pairs to memory slots.\n",
    "\n",
    "        Writing strategy:\n",
    "        1. Check if current step is a multiple of update_frequency\n",
    "        2. If using surprise gating, weight writes by prediction error\n",
    "        3. Replace oldest slots or use DGD-style adaptive update\n",
    "\n",
    "        Args:\n",
    "            keys_new: New keys to write (batch, key_dim)\n",
    "            values_new: New values to write (batch, value_dim)\n",
    "            surprise: Surprise scores for gating (batch,)\n",
    "        \"\"\"\n",
    "        # Check if we should update at this step\n",
    "        should_update = (self.step_counter % self.update_frequency) == 0\n",
    "\n",
    "        if not should_update:\n",
    "            self.step_counter += 1\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_size = keys_new.shape[0]\n",
    "\n",
    "            # Apply surprise gating if enabled\n",
    "            if self.use_surprise_gating:\n",
    "                # Higher surprise \u2192 higher write weight\n",
    "                write_weights = torch.sigmoid(surprise - surprise.mean())  # (batch_size,)\n",
    "            else:\n",
    "                write_weights = torch.ones(batch_size, device=keys_new.device)  # (batch_size,)\n",
    "\n",
    "            # Find slots to replace (oldest slots)\n",
    "            num_to_replace = min(batch_size, self.num_slots)\n",
    "            _, oldest_indices = torch.topk(self.slot_age, num_to_replace, largest=True)  # oldest_indices is (num_to_replace,)\n",
    "\n",
    "            # Select the keys/values/weights that will be written\n",
    "            # These are the first `num_to_replace` items from the incoming batch\n",
    "            keys_to_write = keys_new[:num_to_replace]\n",
    "            values_to_write = values_new[:num_to_replace]\n",
    "            weights_to_apply = write_weights[:num_to_replace]\n",
    "\n",
    "            # Prepare decay tensor for broadcasting\n",
    "            # decay_tensor will be (num_to_replace, 1) to broadcast across key_dim\n",
    "            decay_tensor = self.memory_decay * (1 - weights_to_apply).unsqueeze(-1)  # (num_to_replace, 1)\n",
    "\n",
    "            if self.use_dgd:\n",
    "                # DGD-style update: blend old and new with adaptive decay\n",
    "                updated_keys = decay_tensor * self.keys[oldest_indices] + (1 - decay_tensor) * keys_to_write\n",
    "                updated_values = decay_tensor * self.values[oldest_indices] + (1 - decay_tensor) * values_to_write\n",
    "\n",
    "                self.keys.data.index_copy_(0, oldest_indices, updated_keys)\n",
    "                self.values.data.index_copy_(0, oldest_indices, updated_values)\n",
    "            else:\n",
    "                # Direct replacement\n",
    "                self.keys.data.index_copy_(0, oldest_indices, keys_to_write)\n",
    "                self.values.data.index_copy_(0, oldest_indices, values_to_write)\n",
    "\n",
    "            # Reset age for these slots\n",
    "            self.slot_age[oldest_indices] = 0\n",
    "\n",
    "            # Age all other slots\n",
    "            self.slot_age += 1\n",
    "            self.step_counter += 1\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset memory to initial state\"\"\"\n",
    "        # Ensure parameters are modified in-place or reassigned as Parameters\n",
    "        with torch.no_grad(): # Operations on parameters should often be wrapped in no_grad\n",
    "            self.keys.normal_()\n",
    "            self.values.normal_()\n",
    "        self.step_counter.zero_()\n",
    "        self.slot_age.zero_()\n",
    "\n",
    "    def get_memory_state(self) -> dict:\n",
    "        \"\"\"Get current memory state for inspection/saving\"\"\"\n",
    "        return {\n",
    "            'keys': self.keys.clone().detach(), # Detach to prevent accidental gradient tracking\n",
    "            'values': self.values.clone().detach(),\n",
    "            'step_counter': self.step_counter.item(),\n",
    "            'slot_age': self.slot_age.clone().detach(),\n",
    "        }\n",
    "\n",
    "    def load_memory_state(self, state: dict) -> None:\n",
    "        \"\"\"Load memory state from checkpoint\"\"\"\n",
    "        self.keys.data.copy_(state['keys']) # Use .data to update nn.Parameter directly\n",
    "        self.values.data.copy_(state['values'])\n",
    "        self.step_counter.copy_(torch.tensor(state['step_counter']))\n",
    "        self.slot_age.copy_(state['slot_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b22354f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "1b22354f",
    "outputId": "bccbf139-b609-4967-b9b3-c2222ec29301"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nContinuum Memory System (CMS) - Multi-timescale knowledge hierarchy.\\n\\nThe CMS is a spectrum of MLP blocks operating at different update frequencies,\\ncreating a knowledge cascade from fast microstructure patterns to slow macro regimes.\\n\\nTheoretical Background:\\n    In traditional neural networks, all parameters update at the same rate. In NSAM,\\n    different parts of the network operate as nested optimization problems with\\n    different timescales. The CMS embodies this by creating memory levels that\\n    update at exponentially spaced intervals:\\n    \\n    Level 1 (Fastest):   Updates every 1 step     \u2192 Microstructure (ticks-minutes)\\n    Level 2:             Updates every 10 steps    \u2192 Intraday dynamics (hours-days)\\n    Level 3:             Updates every 100 steps   \u2192 Medium-term patterns (days-weeks)\\n    Level 4 (Slowest):   Updates every 1000 steps  \u2192 Macro regimes (weeks-months)\\n    \\n    Knowledge cascades: When fast blocks update, slow blocks retain previous knowledge.\\n    This creates anti-catastrophic-forgetting: macro knowledge persists even as\\n    microstructure adapts rapidly.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# ============= MODEL - CONTINUUM MEMORY =============\n",
    "\"\"\"\n",
    "Continuum Memory System (CMS) - Multi-timescale knowledge hierarchy.\n",
    "\n",
    "The CMS is a spectrum of MLP blocks operating at different update frequencies,\n",
    "creating a knowledge cascade from fast microstructure patterns to slow macro regimes.\n",
    "\n",
    "Theoretical Background:\n",
    "    In traditional neural networks, all parameters update at the same rate. In NSAM,\n",
    "    different parts of the network operate as nested optimization problems with\n",
    "    different timescales. The CMS embodies this by creating memory levels that\n",
    "    update at exponentially spaced intervals:\n",
    "\n",
    "    Level 1 (Fastest):   Updates every 1 step     \u2192 Microstructure (ticks-minutes)\n",
    "    Level 2:             Updates every 10 steps    \u2192 Intraday dynamics (hours-days)\n",
    "    Level 3:             Updates every 100 steps   \u2192 Medium-term patterns (days-weeks)\n",
    "    Level 4 (Slowest):   Updates every 1000 steps  \u2192 Macro regimes (weeks-months)\n",
    "\n",
    "    Knowledge cascades: When fast blocks update, slow blocks retain previous knowledge.\n",
    "    This creates anti-catastrophic-forgetting: macro knowledge persists even as\n",
    "    microstructure adapts rapidly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "923946ca",
   "metadata": {
    "id": "923946ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dcbf625",
   "metadata": {
    "id": "1dcbf625"
   },
   "outputs": [],
   "source": [
    "class ContinuumMemoryLevel(nn.Module):\n",
    "    \"\"\"\n",
    "    A single level in the continuum memory hierarchy.\n",
    "\n",
    "    Each level is an MLP block with:\n",
    "    - Its own update frequency\n",
    "    - Residual connections\n",
    "    - Layer normalization\n",
    "    - Knowledge cascade from faster levels\n",
    "\n",
    "    Args:\n",
    "        input_dim: Input dimension\n",
    "        hidden_dim: Hidden dimension for this level\n",
    "        update_frequency: How often this level updates its parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        update_frequency: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.update_frequency = update_frequency\n",
    "\n",
    "        # MLP block\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Projection if input_dim != hidden_dim\n",
    "        self.input_proj = None\n",
    "        if input_dim != hidden_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Step counter for update scheduling\n",
    "        self.register_buffer('step_counter', torch.tensor(0, dtype=torch.long))\n",
    "\n",
    "        # Cached output from last update (for when not updating)\n",
    "        self.register_buffer('cached_output', None)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        force_update: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with conditional updating.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, input_dim) or (batch, input_dim)\n",
    "            force_update: If True, force update regardless of frequency\n",
    "\n",
    "        Returns:\n",
    "            output: Processed tensor (batch, seq_len, hidden_dim) or (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        should_update = force_update or ((self.step_counter % self.update_frequency) == 0)\n",
    "\n",
    "        if should_update or self.cached_output is None:\n",
    "            # Compute new output\n",
    "            if self.input_proj is not None:\n",
    "                identity = self.input_proj(x)\n",
    "            else:\n",
    "                identity = x\n",
    "\n",
    "            output = self.mlp(x) + identity\n",
    "\n",
    "            # Cache for future non-update steps\n",
    "            # Only cache if batch size is 1 or we're in eval mode\n",
    "            if self.training and x.shape[0] == 1:\n",
    "                self.cached_output = output.detach()\n",
    "\n",
    "            self.step_counter += 1\n",
    "            return output\n",
    "        else:\n",
    "            # Return cached output\n",
    "            # This implements the \"slow memory\" concept: the level maintains\n",
    "            # its previous knowledge without updating\n",
    "            self.step_counter += 1\n",
    "\n",
    "            # If shapes match, use cached; otherwise recompute\n",
    "            if self.cached_output is not None and self.cached_output.shape == x.shape:\n",
    "                return self.cached_output\n",
    "            else:\n",
    "                # Fallback: recompute if shapes don't match\n",
    "                if self.input_proj is not None:\n",
    "                    identity = self.input_proj(x)\n",
    "                else:\n",
    "                    identity = x\n",
    "                return self.mlp(x) + identity\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset step counter and cache\"\"\"\n",
    "        self.step_counter.zero_()\n",
    "        self.cached_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9117fc",
   "metadata": {
    "id": "8b9117fc"
   },
   "outputs": [],
   "source": [
    "class ContinuumMemorySystem(nn.Module):\n",
    "    \"\"\"\n",
    "    A spectrum of MLP blocks at different update frequencies.\n",
    "\n",
    "    This creates a hierarchy of memories matching forex market dynamics:\n",
    "    - Fast levels capture microstructure (spread widening, momentum bursts)\n",
    "    - Medium levels capture intraday patterns (session effects, news reactions)\n",
    "    - Slow levels capture macro regimes (carry trade, risk-on/risk-off)\n",
    "\n",
    "    Knowledge flows bidirectionally:\n",
    "    - Bottom-up: Fast patterns inform slow regime detection\n",
    "    - Top-down: Slow regimes modulate fast pattern interpretation\n",
    "\n",
    "    Args:\n",
    "        input_dim: Input feature dimension\n",
    "        hidden_dim: Hidden dimension for MLP blocks\n",
    "        num_levels: Number of memory levels\n",
    "        base_frequency: Update frequency for fastest level (default: 1)\n",
    "        frequency_multiplier: Multiplier between levels (default: 10)\n",
    "        hidden_dims: Optional list of hidden dims per level (default: all same)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_levels: int = 4,\n",
    "        base_frequency: int = 1,\n",
    "        frequency_multiplier: int = 10,\n",
    "        hidden_dims: Optional[List[int]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_levels = num_levels\n",
    "        self.base_frequency = base_frequency\n",
    "        self.frequency_multiplier = frequency_multiplier\n",
    "\n",
    "        # Determine hidden dims for each level\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [hidden_dim] * num_levels\n",
    "        else:\n",
    "            assert len(hidden_dims) == num_levels\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Calculate update frequencies for each level\n",
    "        self.update_frequencies = self._calculate_frequencies()\n",
    "\n",
    "        # Create memory levels\n",
    "        self.levels = nn.ModuleList()\n",
    "        current_input_dim = input_dim\n",
    "\n",
    "        for i, (freq, h_dim) in enumerate(zip(self.update_frequencies, hidden_dims)):\n",
    "            level = ContinuumMemoryLevel(\n",
    "                input_dim=current_input_dim,\n",
    "                hidden_dim=h_dim,\n",
    "                update_frequency=freq,\n",
    "            )\n",
    "            self.levels.append(level)\n",
    "            current_input_dim = h_dim  # Output of this level feeds to next\n",
    "\n",
    "        # Cross-level fusion: combine all levels for final output\n",
    "        total_dim = sum(hidden_dims)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def _calculate_frequencies(self) -> List[int]:\n",
    "        \"\"\"Calculate exponentially spaced update frequencies\"\"\"\n",
    "        frequencies = []\n",
    "        for i in range(self.num_levels):\n",
    "            freq = self.base_frequency * (self.frequency_multiplier ** i)\n",
    "            frequencies.append(freq)\n",
    "        return frequencies\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        return_all_levels: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through all memory levels.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, input_dim) or (batch, input_dim)\n",
    "            return_all_levels: If True, return outputs from all levels\n",
    "\n",
    "        Returns:\n",
    "            If return_all_levels=False:\n",
    "                Fused output from all levels (batch, seq_len, hidden_dim) or (batch, hidden_dim)\n",
    "            If return_all_levels=True:\n",
    "                Tuple of (fused_output, level_outputs_list)\n",
    "        \"\"\"\n",
    "        level_outputs = []\n",
    "        current_input = x\n",
    "\n",
    "        # Process through each level sequentially (knowledge cascade)\n",
    "        for i, level in enumerate(self.levels):\n",
    "            level_out = level(current_input)\n",
    "            level_outputs.append(level_out)\n",
    "            current_input = level_out  # Feed to next level\n",
    "\n",
    "        # Fuse all levels\n",
    "        # Each level captures patterns at different timescales\n",
    "        fused = torch.cat(level_outputs, dim=-1)\n",
    "        output = self.fusion(fused)\n",
    "\n",
    "        if return_all_levels:\n",
    "            return output, level_outputs\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def get_level_states(self) -> List[dict]:\n",
    "        \"\"\"Get state of each level for inspection\"\"\"\n",
    "        states = []\n",
    "        for i, level in enumerate(self.levels):\n",
    "            states.append({\n",
    "                'level': i,\n",
    "                'update_frequency': level.update_frequency,\n",
    "                'step_counter': level.step_counter.item(),\n",
    "                'has_cached_output': level.cached_output is not None,\n",
    "            })\n",
    "        return states\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset all memory levels\"\"\"\n",
    "        for level in self.levels:\n",
    "            level.reset()\n",
    "\n",
    "    def get_slowest_level_output(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get output from the slowest (macro regime) level only.\n",
    "\n",
    "        This is useful for regime detection, as the slowest level contains\n",
    "        the most persistent, macro-scale knowledge.\n",
    "        \"\"\"\n",
    "        current_input = x\n",
    "        for level in self.levels:\n",
    "            current_input = level(current_input)\n",
    "        return current_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa898928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "fa898928",
    "outputId": "8993eba0-afae-47eb-93a6-f8ada3909b8e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nCross-Pair Associative Memory - Learning currency pair correlations.\\n\\nForex pairs don't trade independently. EUR/USD movement creates associative\\npatterns in GBP/USD, USD/JPY, etc. This module learns these correlations\\nas nested associative memories.\\n\\nTheoretical Background:\\n    In NSAM, memories can be composed hierarchically. Cross-pair memory\\n    treats each pair's hidden state as a key, and retrieves associated\\n    movements in other pairs. This captures:\\n    \\n    - Currency correlations (EUR/USD \u2194 GBP/USD positive)\\n    - Inverse relationships (EUR/USD \u2194 USD/JPY negative)\\n    - Safe-haven flows (Risk-off \u2192 JPY up, carry pairs down)\\n    - Macro context (Gold/yields affecting all USD pairs)\\n\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# ============= MODEL - CROSS-PAIR MEMORY =============\n",
    "\"\"\"\n",
    "Cross-Pair Associative Memory - Learning currency pair correlations.\n",
    "\n",
    "Forex pairs don't trade independently. EUR/USD movement creates associative\n",
    "patterns in GBP/USD, USD/JPY, etc. This module learns these correlations\n",
    "as nested associative memories.\n",
    "\n",
    "Theoretical Background:\n",
    "    In NSAM, memories can be composed hierarchically. Cross-pair memory\n",
    "    treats each pair's hidden state as a key, and retrieves associated\n",
    "    movements in other pairs. This captures:\n",
    "\n",
    "    - Currency correlations (EUR/USD \u2194 GBP/USD positive)\n",
    "    - Inverse relationships (EUR/USD \u2194 USD/JPY negative)\n",
    "    - Safe-haven flows (Risk-off \u2192 JPY up, carry pairs down)\n",
    "    - Macro context (Gold/yields affecting all USD pairs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e51f99b",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "0e51f99b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d7eaab9",
   "metadata": {
    "id": "3d7eaab9"
   },
   "outputs": [],
   "source": [
    "class CrossPairMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns correlations between currency pairs as associative memories.\n",
    "\n",
    "    Also ingests macro features (yields, commodities, rates) as context keys\n",
    "    that affect all pairs simultaneously.\n",
    "\n",
    "    Args:\n",
    "        num_pairs: Number of currency pairs\n",
    "        pair_dim: Dimension of per-pair representations\n",
    "        macro_dim: Dimension of macro feature encoding\n",
    "        num_correlation_slots: Number of correlation pattern slots\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_pairs: int,\n",
    "        pair_dim: int,\n",
    "        macro_dim: int,\n",
    "        num_correlation_slots: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_pairs = num_pairs\n",
    "        self.pair_dim = pair_dim\n",
    "        self.macro_dim = macro_dim\n",
    "\n",
    "        # Pair-to-pair correlation memory\n",
    "        # Key: pair_i state, Value: expected correlated movements in other pairs\n",
    "        self.pair_correlation_memory = AssociativeMemory(\n",
    "            key_dim=pair_dim,\n",
    "            value_dim=pair_dim * num_pairs,\n",
    "            num_slots=num_correlation_slots,\n",
    "            update_frequency=5,  # Medium-term correlations\n",
    "            use_surprise_gating=True,\n",
    "        )\n",
    "\n",
    "        # Macro-to-pairs memory\n",
    "        # Key: macro state (yields, VIX, etc.), Value: expected pair responses\n",
    "        self.macro_memory = AssociativeMemory(\n",
    "            key_dim=macro_dim,\n",
    "            value_dim=pair_dim * num_pairs,\n",
    "            num_slots=num_correlation_slots,\n",
    "            update_frequency=10,  # Slower, macro regimes\n",
    "            use_surprise_gating=True,\n",
    "        )\n",
    "\n",
    "        # Fusion layer to combine correlation signals\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(pair_dim * num_pairs * 2, pair_dim * num_pairs),\n",
    "            nn.LayerNorm(pair_dim * num_pairs),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(pair_dim * num_pairs, pair_dim * num_pairs),\n",
    "        )\n",
    "\n",
    "        # Per-pair output projections\n",
    "        self.pair_outputs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(pair_dim * 2, pair_dim),\n",
    "                nn.LayerNorm(pair_dim),\n",
    "            )\n",
    "            for _ in range(num_pairs)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pair_states: torch.Tensor,\n",
    "        macro_state: torch.Tensor,\n",
    "        write_mode: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: compute cross-pair correlations.\n",
    "\n",
    "        Args:\n",
    "            pair_states: Per-pair hidden states (batch, num_pairs, pair_dim)\n",
    "            macro_state: Macro feature encoding (batch, macro_dim)\n",
    "            write_mode: Whether to write to correlation memories\n",
    "\n",
    "        Returns:\n",
    "            enriched_states: States enriched with correlation info (batch, num_pairs, pair_dim)\n",
    "        \"\"\"\n",
    "        batch_size = pair_states.shape[0]\n",
    "\n",
    "        # Flatten pair states for correlation lookup\n",
    "        # Use mean across pairs as the query key\n",
    "        pair_query = pair_states.mean(dim=1)  # (batch, pair_dim)\n",
    "\n",
    "        # Retrieve pair-to-pair correlations\n",
    "        pair_correlations, pair_surprise = self.pair_correlation_memory(\n",
    "            query=pair_query,\n",
    "            value_target=None,\n",
    "            write_mode=False,\n",
    "        )\n",
    "\n",
    "        # Retrieve macro-driven correlations\n",
    "        macro_correlations, macro_surprise = self.macro_memory(\n",
    "            query=macro_state,\n",
    "            value_target=None,\n",
    "            write_mode=False,\n",
    "        )\n",
    "\n",
    "        # Fuse correlation signals\n",
    "        fused_correlations = self.fusion(\n",
    "            torch.cat([pair_correlations, macro_correlations], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Reshape to (batch, num_pairs, pair_dim)\n",
    "        fused_correlations = fused_correlations.view(batch_size, self.num_pairs, self.pair_dim)\n",
    "\n",
    "        # Combine with original pair states\n",
    "        enriched_states = []\n",
    "        for i in range(self.num_pairs):\n",
    "            pair_input = torch.cat([\n",
    "                pair_states[:, i, :],\n",
    "                fused_correlations[:, i, :],\n",
    "            ], dim=-1)\n",
    "            enriched = self.pair_outputs[i](pair_input)\n",
    "            enriched_states.append(enriched)\n",
    "\n",
    "        enriched_states = torch.stack(enriched_states, dim=1)\n",
    "\n",
    "        # Write to memories if enabled\n",
    "        if write_mode:\n",
    "            # Write observed correlations\n",
    "            actual_correlations = pair_states.view(batch_size, -1)\n",
    "            self.pair_correlation_memory._write_to_memory(\n",
    "                pair_query,\n",
    "                actual_correlations,\n",
    "                pair_surprise,\n",
    "            )\n",
    "            self.macro_memory._write_to_memory(\n",
    "                macro_state,\n",
    "                actual_correlations,\n",
    "                macro_surprise,\n",
    "            )\n",
    "\n",
    "        return enriched_states\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset all correlation memories\"\"\"\n",
    "        self.pair_correlation_memory.reset()\n",
    "        self.macro_memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "428047d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "428047d9",
    "outputId": "6c5bf716-a2ec-4e5b-8dba-ac22794a89af"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nRegime Detector - Latent market state classification.\\n\\nDetects whether the market is in trending, ranging, volatile, or quiet regimes.\\nUses the slow CMS blocks as input, since regime = slow macro knowledge.\\n\\nRegime detection feeds back into the model to modulate predictions.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# ============= MODEL - REGIME DETECTOR =============\n",
    "\"\"\"\n",
    "Regime Detector - Latent market state classification.\n",
    "\n",
    "Detects whether the market is in trending, ranging, volatile, or quiet regimes.\n",
    "Uses the slow CMS blocks as input, since regime = slow macro knowledge.\n",
    "\n",
    "Regime detection feeds back into the model to modulate predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41670bb7",
   "metadata": {
    "id": "41670bb7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6ceeb55",
   "metadata": {
    "id": "a6ceeb55"
   },
   "outputs": [],
   "source": [
    "class RegimeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Detects latent market regimes from slow memory blocks.\n",
    "\n",
    "    Regimes:\n",
    "    0: Trending (directional movement, follow momentum)\n",
    "    1: Ranging (mean reversion, fade extremes)\n",
    "    2: Volatile (high uncertainty, reduce position size)\n",
    "    3: Quiet (low volume, widen stops, reduce frequency)\n",
    "\n",
    "    The regime is detected from the slowest CMS level, which contains\n",
    "    the most persistent macro knowledge.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Input dimension (from slow CMS level)\n",
    "        hidden_dim: Hidden dimension for regime classifier\n",
    "        num_regimes: Number of regime classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        num_regimes: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_regimes = num_regimes\n",
    "\n",
    "        # Regime feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Regime classifier head\n",
    "        self.classifier = nn.Linear(hidden_dim, num_regimes)\n",
    "\n",
    "        # Regime embedding (for feeding back to the model)\n",
    "        self.regime_embeddings = nn.Embedding(num_regimes, hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        slow_memory_state: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Detect market regime.\n",
    "\n",
    "        Args:\n",
    "            slow_memory_state: Output from slowest CMS level (batch, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            regime_logits: Logits for each regime class (batch, num_regimes)\n",
    "            regime_features: Regime feature embedding (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        regime_features = self.feature_extractor(slow_memory_state)\n",
    "\n",
    "        # Classify regime\n",
    "        regime_logits = self.classifier(regime_features)\n",
    "\n",
    "        return regime_logits, regime_features\n",
    "\n",
    "    def get_regime_embedding(\n",
    "        self,\n",
    "        regime_probs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get soft regime embedding based on regime probabilities.\n",
    "\n",
    "        Args:\n",
    "            regime_probs: Probabilities for each regime (batch, num_regimes)\n",
    "\n",
    "        Returns:\n",
    "            regime_emb: Soft regime embedding (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Get all regime embeddings\n",
    "        all_embeddings = self.regime_embeddings.weight  # (num_regimes, hidden_dim)\n",
    "\n",
    "        # Soft combination based on probabilities\n",
    "        regime_emb = torch.matmul(regime_probs, all_embeddings)  # (batch, hidden_dim)\n",
    "\n",
    "        return regime_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7ab3a94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "b7ab3a94",
    "outputId": "65fe9f09-a38d-4332-fc91-72b8581be5a7"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nSession-Aware Frequency Gate - Adaptive memory activation.\\n\\nForex markets exhibit session-dependent dynamics. During London-NY overlap,\\nvolatility spikes and fast memories should dominate. During Asian session,\\nslow memories better capture the ranging behavior.\\n\\nThis module gates memory levels based on active trading sessions.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# ============= MODEL - SESSION GATE =============\n",
    "\"\"\"\n",
    "Session-Aware Frequency Gate - Adaptive memory activation.\n",
    "\n",
    "Forex markets exhibit session-dependent dynamics. During London-NY overlap,\n",
    "volatility spikes and fast memories should dominate. During Asian session,\n",
    "slow memories better capture the ranging behavior.\n",
    "\n",
    "This module gates memory levels based on active trading sessions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3237d6d",
   "metadata": {
    "id": "b3237d6d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddf5679b",
   "metadata": {
    "id": "ddf5679b"
   },
   "outputs": [],
   "source": [
    "class SessionFrequencyGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gates memory levels based on active forex session.\n",
    "\n",
    "    Session characteristics:\n",
    "    - Sydney: Lowest volume, ranging\n",
    "    - Tokyo: Medium volume, trend following\n",
    "    - London: High volume, breakouts\n",
    "    - New York: Highest volume, reversals\n",
    "    - London-NY overlap: Extreme volatility, fast memories crucial\n",
    "\n",
    "    Also responds to news events, which spike all frequencies.\n",
    "\n",
    "    Args:\n",
    "        num_memory_levels: Number of CMS levels to gate\n",
    "        session_embedding_dim: Dimension of session embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_memory_levels: int = 4,\n",
    "        session_embedding_dim: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_memory_levels = num_memory_levels\n",
    "        self.session_embedding_dim = session_embedding_dim\n",
    "\n",
    "        # Session encoder: maps session indicators to embeddings\n",
    "        # Input: [is_sydney, is_tokyo, is_london, is_ny, is_overlap, is_news_event]\n",
    "        self.session_encoder = nn.Sequential(\n",
    "            nn.Linear(6, session_embedding_dim),\n",
    "            nn.LayerNorm(session_embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(session_embedding_dim, session_embedding_dim),\n",
    "        )\n",
    "\n",
    "        # Frequency gate generator\n",
    "        # Outputs logits for each memory level's activation\n",
    "        self.gate_generator = nn.Sequential(\n",
    "            nn.Linear(session_embedding_dim, num_memory_levels * 2),\n",
    "            nn.LayerNorm(num_memory_levels * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(num_memory_levels * 2, num_memory_levels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        session_indicators: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate frequency gates for memory levels.\n",
    "\n",
    "        Args:\n",
    "            session_indicators: Session state (batch, 6)\n",
    "                [is_sydney, is_tokyo, is_london, is_ny, is_overlap, is_news_event]\n",
    "\n",
    "        Returns:\n",
    "            gates: Activation gates for each level (batch, num_memory_levels)\n",
    "                Range: [0, 1] via sigmoid, higher = more active\n",
    "        \"\"\"\n",
    "        # Encode session\n",
    "        session_emb = self.session_encoder(session_indicators)\n",
    "\n",
    "        # Generate gates\n",
    "        gate_logits = self.gate_generator(session_emb)\n",
    "\n",
    "        # Sigmoid to [0, 1] range\n",
    "        gates = torch.sigmoid(gate_logits)\n",
    "\n",
    "        # During news events (last indicator), boost all gates\n",
    "        is_news = session_indicators[:, -1:].unsqueeze(-1)  # (batch, 1, 1)\n",
    "        gates = gates + is_news * 0.5  # Boost by 0.5 during news\n",
    "        gates = torch.clamp(gates, 0, 1)\n",
    "\n",
    "        return gates\n",
    "\n",
    "    def apply_gates(\n",
    "        self,\n",
    "        level_outputs: list,\n",
    "        gates: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply gates to memory level outputs.\n",
    "\n",
    "        Args:\n",
    "            level_outputs: List of tensors from each CMS level\n",
    "            gates: Gates for each level (batch, num_memory_levels)\n",
    "\n",
    "        Returns:\n",
    "            gated_outputs: Weighted combination of levels (batch, ...)\n",
    "        \"\"\"\n",
    "        # Normalize gates to sum to 1 (softmax across levels)\n",
    "        gate_weights = F.softmax(gates, dim=-1)\n",
    "\n",
    "        # Weight each level's output\n",
    "        gated = []\n",
    "        for i, output in enumerate(level_outputs):\n",
    "            weight = gate_weights[:, i].unsqueeze(-1)\n",
    "            # Expand weight to match output shape\n",
    "            while weight.dim() < output.dim():\n",
    "                weight = weight.unsqueeze(-1)\n",
    "            gated.append(output * weight)\n",
    "\n",
    "        # Sum weighted outputs\n",
    "        gated_output = torch.stack(gated, dim=0).sum(dim=0)\n",
    "\n",
    "        return gated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5bc0db2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "f5bc0db2",
    "outputId": "b0d8b8fa-ddca-437f-a901-9e0b8e337388"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nSelf-Modifying Titans - In-context adaptive sequence processing.\\n\\nUnlike standard Transformers, this module generates its OWN keys, values,\\nand learning rates during the forward pass. This enables deep self-modification:\\nthe model adapts its memory writing strategy based on the current market regime.\\n\\nTheoretical Background:\\n    In NSAM, each optimization problem can generate its own learning rate schedule.\\n    Applied to forex: during high-volatility events (NFP, rate decisions), the model\\n    should \"pay more attention\" by increasing its in-context learning rate.\\n    \\n    The Titans architecture processes sequences while maintaining a persistent\\n    associative memory. At each timestep:\\n    1. Generate query from current input\\n    2. Read from memory using the query\\n    3. Generate new (key, value, lr) triple based on current state\\n    4. Write to memory with self-generated learning rate\\n    5. Output prediction\\n    \\n    This allows the model to adapt without weight updates, crucial for live trading\\n    where we can\\'t retrain on each tick.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# ============= MODEL - SELF-MODIFYING TITANS =============\n",
    "\"\"\"\n",
    "Self-Modifying Titans - In-context adaptive sequence processing.\n",
    "\n",
    "Unlike standard Transformers, this module generates its OWN keys, values,\n",
    "and learning rates during the forward pass. This enables deep self-modification:\n",
    "the model adapts its memory writing strategy based on the current market regime.\n",
    "\n",
    "Theoretical Background:\n",
    "    In NSAM, each optimization problem can generate its own learning rate schedule.\n",
    "    Applied to forex: during high-volatility events (NFP, rate decisions), the model\n",
    "    should \"pay more attention\" by increasing its in-context learning rate.\n",
    "\n",
    "    The Titans architecture processes sequences while maintaining a persistent\n",
    "    associative memory. At each timestep:\n",
    "    1. Generate query from current input\n",
    "    2. Read from memory using the query\n",
    "    3. Generate new (key, value, lr) triple based on current state\n",
    "    4. Write to memory with self-generated learning rate\n",
    "    5. Output prediction\n",
    "\n",
    "    This allows the model to adapt without weight updates, crucial for live trading\n",
    "    where we can't retrain on each tick.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca02160d",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "ca02160d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81660901",
   "metadata": {
    "id": "81660901"
   },
   "outputs": [],
   "source": [
    "class SelfModifyingTitansLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of Self-Modifying Titans.\n",
    "\n",
    "    Processes input through:\n",
    "    1. Query generation\n",
    "    2. Memory read\n",
    "    3. Key-value-lr generation\n",
    "    4. Memory write\n",
    "    5. Output generation\n",
    "\n",
    "    Args:\n",
    "        input_dim: Input dimension\n",
    "        hidden_dim: Hidden dimension\n",
    "        num_memory_slots: Number of slots in the associative memory\n",
    "        memory_update_frequency: How often the memory updates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_memory_slots: int = 128,\n",
    "        memory_update_frequency: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Query generation\n",
    "        self.query_gen = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Associative memory\n",
    "        self.memory = AssociativeMemory(\n",
    "            key_dim=hidden_dim,\n",
    "            value_dim=hidden_dim,\n",
    "            num_slots=num_memory_slots,\n",
    "            update_frequency=memory_update_frequency,\n",
    "            use_surprise_gating=True,\n",
    "            use_dgd=True,\n",
    "        )\n",
    "\n",
    "        # Key-Value-LR generator\n",
    "        # Generates new memory entries and learning rate from current state\n",
    "        self.kv_lr_gen = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 3),  # 2x input \u2192 3x output (K, V, LR)\n",
    "            nn.LayerNorm(hidden_dim * 3),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        write_mode: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Forward pass through the Titans layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, input_dim) or (batch, input_dim)\n",
    "            write_mode: Whether to write to memory\n",
    "\n",
    "        Returns:\n",
    "            output: Processed tensor (batch, seq_len, hidden_dim) or (batch, hidden_dim)\n",
    "            aux_info: Dictionary with auxiliary information (surprise, lr, etc.)\n",
    "        \"\"\"\n",
    "        # Handle both 2D and 3D input\n",
    "        is_3d = x.dim() == 3\n",
    "        \n",
    "        x_proj = self.input_proj(x)\n",
    "        identity = x_proj\n",
    "        \n",
    "        # For memory operations, use last timestep if 3D\n",
    "        # Memory expects 2D tensors (batch, dim), so we extract the final timestep\n",
    "        # which represents the most recent state in the sequence.\n",
    "        # This allows the model to read/write memory based on current state\n",
    "        # while still processing the full sequence for output generation.\n",
    "        if is_3d:\n",
    "            query_for_mem = self.query_gen(x_proj[:, -1, :])  # (batch, hidden_dim)\n",
    "        else:\n",
    "            query_for_mem = self.query_gen(x_proj)\n",
    "        \n",
    "        # Read from memory (2D query)\n",
    "        memory_out, surprise = self.memory(query_for_mem, write_mode=False)\n",
    "        \n",
    "        # Expand memory output back to match sequence if needed\n",
    "        if is_3d:\n",
    "            seq_len = x.shape[1]\n",
    "            memory_out_expanded = memory_out.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "            combined = torch.cat([x_proj, memory_out_expanded], dim=-1)\n",
    "        else:\n",
    "            combined = torch.cat([x_proj, memory_out], dim=-1)\n",
    "        \n",
    "        kv_lr = self.kv_lr_gen(combined)\n",
    "        key_new, value_new, lr_logit = torch.chunk(kv_lr, 3, dim=-1)\n",
    "        \n",
    "        if is_3d:\n",
    "            lr = torch.sigmoid(lr_logit[:, -1, :]).mean(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            lr = torch.sigmoid(lr_logit).mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Write to memory with 2D tensors\n",
    "        if write_mode:\n",
    "            if is_3d:\n",
    "                self.memory._write_to_memory(key_new[:, -1, :], value_new[:, -1, :], surprise)\n",
    "            else:\n",
    "                self.memory._write_to_memory(key_new, value_new, surprise)\n",
    "        \n",
    "        output = self.output_proj(combined)\n",
    "        output = self.norm1(output + identity)\n",
    "        \n",
    "        aux_info = {\n",
    "            'surprise': surprise,\n",
    "            'learning_rate': lr,\n",
    "            'memory_retrieval': memory_out,\n",
    "        }\n",
    "        return output, aux_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e6a827b",
   "metadata": {
    "id": "3e6a827b"
   },
   "outputs": [],
   "source": [
    "class SelfModifyingTitans(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Self-Modifying Titans for sequence processing.\n",
    "\n",
    "    Stacks multiple Titans layers with residual connections, creating\n",
    "    a deep hierarchy of self-modifying memories.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Input feature dimension\n",
    "        hidden_dim: Hidden dimension for all layers\n",
    "        num_memory_slots: Number of slots per memory\n",
    "        num_layers: Number of Titans layers\n",
    "        memory_update_frequency: Base update frequency for memories\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_memory_slots: int = 128,\n",
    "        num_layers: int = 4,\n",
    "        memory_update_frequency: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Create layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = input_dim\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            layer = SelfModifyingTitansLayer(\n",
    "                input_dim=current_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                num_memory_slots=num_memory_slots,\n",
    "                memory_update_frequency=memory_update_frequency * (i + 1),  # Slower at deeper layers\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            current_dim = hidden_dim\n",
    "\n",
    "        # Final output projection\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        write_mode: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Forward pass through all Titans layers.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, input_dim) or (batch, input_dim)\n",
    "            write_mode: Whether to write to memories\n",
    "\n",
    "        Returns:\n",
    "            output: Final output (batch, seq_len, hidden_dim) or (batch, hidden_dim)\n",
    "            all_aux_info: Aggregated auxiliary info from all layers\n",
    "        \"\"\"\n",
    "        all_surprises = []\n",
    "        all_lrs = []\n",
    "        current = x\n",
    "\n",
    "        # Process through layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            current, aux_info = layer(current, write_mode=write_mode)\n",
    "            all_surprises.append(aux_info['surprise'])\n",
    "            all_lrs.append(aux_info['learning_rate'])\n",
    "\n",
    "        # Final normalization\n",
    "        output = self.output_norm(current)\n",
    "\n",
    "        # Aggregate auxiliary information\n",
    "        all_aux_info = {\n",
    "            'surprises': all_surprises,  # List of surprise tensors per layer\n",
    "            'learning_rates': all_lrs,   # List of lr tensors per layer\n",
    "            'mean_surprise': torch.stack(all_surprises).mean(),\n",
    "            'mean_lr': torch.stack(all_lrs).mean(),\n",
    "        }\n",
    "\n",
    "        return output, all_aux_info\n",
    "\n",
    "    def reset_memories(self) -> None:\n",
    "        \"\"\"Reset all memories in all layers\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.memory.reset()\n",
    "\n",
    "    def get_memory_states(self) -> list:\n",
    "        \"\"\"Get states of all memories for checkpointing\"\"\"\n",
    "        states = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            states.append({\n",
    "                'layer': i,\n",
    "                'memory_state': layer.memory.get_memory_state(),\n",
    "            })\n",
    "        return states\n",
    "\n",
    "    def load_memory_states(self, states: list) -> None:\n",
    "        \"\"\"Load memory states from checkpoint\"\"\"\n",
    "        for i, state in enumerate(states):\n",
    "            self.layers[i].memory.load_memory_state(state['memory_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3710ab85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "3710ab85",
    "outputId": "040bd9cd-ac06-42ad-e29e-aba655c221a2"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nOutput Heads - Multi-task prediction outputs.\\n\\nNEXUS-FX predicts multiple targets simultaneously:\\n1. Direction (up/neutral/down classification)\\n2. Volatility (regression on future realized volatility)\\n3. Regime (current regime classification)\\n4. Confidence (uncertainty quantification)\\n\\nMulti-task learning improves generalization and provides richer signals\\nfor trading decisions.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# ============= MODEL - OUTPUT HEADS =============\n",
    "\"\"\"\n",
    "Output Heads - Multi-task prediction outputs.\n",
    "\n",
    "NEXUS-FX predicts multiple targets simultaneously:\n",
    "1. Direction (up/neutral/down classification)\n",
    "2. Volatility (regression on future realized volatility)\n",
    "3. Regime (current regime classification)\n",
    "4. Confidence (uncertainty quantification)\n",
    "\n",
    "Multi-task learning improves generalization and provides richer signals\n",
    "for trading decisions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70b6c8ae",
   "metadata": {
    "id": "70b6c8ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37ca7a46",
   "metadata": {
    "id": "37ca7a46"
   },
   "outputs": [],
   "source": [
    "class OutputHeads(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task prediction heads for NEXUS-FX.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of input features from backbone\n",
    "        num_direction_classes: Number of direction classes (default: 3 for up/neutral/down)\n",
    "        predict_volatility: Whether to predict volatility\n",
    "        predict_regime: Whether to predict regime\n",
    "        output_confidence: Whether to output confidence scores\n",
    "        num_regimes: Number of regime classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_direction_classes: int = 3,\n",
    "        predict_volatility: bool = True,\n",
    "        predict_regime: bool = True,\n",
    "        output_confidence: bool = True,\n",
    "        num_regimes: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_direction_classes = num_direction_classes\n",
    "        self.predict_volatility = predict_volatility\n",
    "        self.predict_regime = predict_regime\n",
    "        self.output_confidence = output_confidence\n",
    "        self.num_regimes = num_regimes\n",
    "\n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Direction prediction head\n",
    "        self.direction_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(input_dim // 2, num_direction_classes),\n",
    "        )\n",
    "\n",
    "        # Volatility prediction head (regression)\n",
    "        if predict_volatility:\n",
    "            self.volatility_head = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(input_dim // 2, 1),\n",
    "                nn.Softplus(),  # Ensure positive volatility predictions\n",
    "            )\n",
    "\n",
    "        # Regime prediction head\n",
    "        if predict_regime:\n",
    "            self.regime_head = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(input_dim // 2, num_regimes),\n",
    "            )\n",
    "\n",
    "        # Confidence head (predicts model uncertainty)\n",
    "        if output_confidence:\n",
    "            self.confidence_head = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(input_dim // 2, 1),\n",
    "                nn.Sigmoid(),  # Confidence in [0, 1]\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generate predictions for all tasks.\n",
    "\n",
    "        Args:\n",
    "            features: Input features (batch, input_dim) or (batch, seq_len, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            outputs: Dictionary containing:\n",
    "                - direction_logits: (batch, num_direction_classes)\n",
    "                - volatility: (batch, 1) if predict_volatility\n",
    "                - regime_logits: (batch, num_regimes) if predict_regime\n",
    "                - confidence: (batch, 1) if output_confidence\n",
    "        \"\"\"\n",
    "        # Shared processing\n",
    "        shared_features = self.shared(features)\n",
    "\n",
    "        outputs = {}\n",
    "\n",
    "        # Direction prediction\n",
    "        outputs['direction_logits'] = self.direction_head(shared_features)\n",
    "\n",
    "        # Volatility prediction\n",
    "        if self.predict_volatility:\n",
    "            outputs['volatility'] = self.volatility_head(shared_features)\n",
    "\n",
    "        # Regime prediction\n",
    "        if self.predict_regime:\n",
    "            outputs['regime_logits'] = self.regime_head(shared_features)\n",
    "\n",
    "        # Confidence prediction\n",
    "        if self.output_confidence:\n",
    "            outputs['confidence'] = self.confidence_head(shared_features)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_predictions(\n",
    "        self,\n",
    "        outputs: Dict[str, torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convert raw outputs to predictions.\n",
    "\n",
    "        Args:\n",
    "            outputs: Raw outputs from forward pass\n",
    "\n",
    "        Returns:\n",
    "            predictions: Dictionary with:\n",
    "                - direction_probs: Softmax probabilities\n",
    "                - direction_class: Predicted class\n",
    "                - volatility: Volatility prediction\n",
    "                - regime_probs: Regime probabilities\n",
    "                - regime_class: Predicted regime\n",
    "                - confidence: Confidence score\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "\n",
    "        # Direction\n",
    "        direction_probs = F.softmax(outputs['direction_logits'], dim=-1)\n",
    "        predictions['direction_probs'] = direction_probs\n",
    "        predictions['direction_class'] = torch.argmax(direction_probs, dim=-1)\n",
    "\n",
    "        # Volatility\n",
    "        if 'volatility' in outputs:\n",
    "            predictions['volatility'] = outputs['volatility']\n",
    "\n",
    "        # Regime\n",
    "        if 'regime_logits' in outputs:\n",
    "            regime_probs = F.softmax(outputs['regime_logits'], dim=-1)\n",
    "            predictions['regime_probs'] = regime_probs\n",
    "            predictions['regime_class'] = torch.argmax(regime_probs, dim=-1)\n",
    "\n",
    "        # Confidence\n",
    "        if 'confidence' in outputs:\n",
    "            predictions['confidence'] = outputs['confidence']\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fefe7eeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "fefe7eeb",
    "outputId": "9a947f72-a965-492c-ab43-ded493d3d5c9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nNEXUS-FX: Main model combining all components.\\n\\nIntegrates:\\n- Multi-timeframe feature processing\\n- Self-Modifying Titans for sequence processing\\n- Continuum Memory System for multi-scale persistence\\n- Cross-Pair Memory for correlation learning\\n- Session-aware frequency gating\\n- Regime detection\\n- Multi-task output heads\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# ============= MODEL - NEXUS-FX (MAIN) =============\n",
    "\"\"\"\n",
    "NEXUS-FX: Main model combining all components.\n",
    "\n",
    "Integrates:\n",
    "- Multi-timeframe feature processing\n",
    "- Self-Modifying Titans for sequence processing\n",
    "- Continuum Memory System for multi-scale persistence\n",
    "- Cross-Pair Memory for correlation learning\n",
    "- Session-aware frequency gating\n",
    "- Regime detection\n",
    "- Multi-task output heads\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faf9011b",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "faf9011b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87f7de2",
   "metadata": {
    "id": "c87f7de2"
   },
   "outputs": [],
   "source": [
    "class NEXUSFX(nn.Module):\n",
    "    \"\"\"\n",
    "    Full NEXUS-FX architecture.\n",
    "\n",
    "    Forward pass flow:\n",
    "    1. Feature engineering: OHLC \u2192 returns, volatility, technicals\n",
    "    2. Macro encoding: calendar events, rates, yields \u2192 macro embedding\n",
    "    3. Session detection: current time \u2192 session embedding\n",
    "    4. Per-pair processing through Self-Modifying Titans\n",
    "    5. Cross-pair correlation via CrossPairMemory\n",
    "    6. Continuum Memory System for multi-scale persistence\n",
    "    7. Session-gated frequency adjustment\n",
    "    8. Regime detection (feeds back to CMS)\n",
    "    9. Output heads: direction, volatility, regime, confidence\n",
    "\n",
    "    Args:\n",
    "        config: NexusFXConfig with all hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: NexusFXConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Feature engineering (not trainable, pure computation)\n",
    "        self.feature_engine = FeatureEngine(\n",
    "            lookback_periods=config.lookback_periods,\n",
    "            include_volume=config.include_volume,\n",
    "        )\n",
    "\n",
    "        # Macro feature encoder\n",
    "        self.macro_encoder = MacroFeatureEncoder(\n",
    "            pairs=config.pairs,\n",
    "            include_calendar=config.include_macro,\n",
    "            include_rates=config.include_macro,\n",
    "            include_yields=config.include_macro,\n",
    "            include_commodities=config.include_macro,\n",
    "            include_sentiment=config.include_macro,\n",
    "        )\n",
    "\n",
    "        # Session clock\n",
    "        self.session_clock = SessionClock()\n",
    "\n",
    "        # Calculate input dimensions\n",
    "        # Features per timeframe: OHLC (4) + technical features (~20)\n",
    "        # A precise count of features from FeatureEngine: 2 (returns) + 3 (volatility) + 1 (RSI) + 3 (MACD) + 5 (BB) + 1 (ATR) + 1 (ADX) + 3 (momentum) = 19\n",
    "        # If include_volume is True, add 3 more features: 19 + 3 = 22\n",
    "        features_per_tf = 22  # Corrected based on FeatureEngine output\n",
    "        num_timeframes = len(config.timeframes)\n",
    "        total_feature_dim = features_per_tf * num_timeframes\n",
    "\n",
    "        # Input projection: map all features to input_dim\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, config.input_dim),\n",
    "            nn.LayerNorm(config.input_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Macro projection\n",
    "        macro_feature_dim = self.macro_encoder.feature_dim\n",
    "        self.macro_projection = nn.Sequential(\n",
    "            nn.Linear(macro_feature_dim, config.input_dim // 2),\n",
    "            nn.LayerNorm(config.input_dim // 2),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Session projection\n",
    "        session_feature_dim = 19  # From session_clock.compute_session_features\n",
    "        self.session_projection = nn.Sequential(\n",
    "            nn.Linear(session_feature_dim, config.session_embedding_dim),\n",
    "            nn.LayerNorm(config.session_embedding_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Per-pair Self-Modifying Titans\n",
    "        self.titans_per_pair = nn.ModuleList([\n",
    "            SelfModifyingTitans(\n",
    "                input_dim=config.input_dim + config.input_dim // 2,  # features + macro\n",
    "                hidden_dim=config.hidden_dim,\n",
    "                num_memory_slots=config.num_memory_slots,\n",
    "                num_layers=config.num_titans_layers,\n",
    "            )\n",
    "            for _ in range(config.num_pairs)\n",
    "        ])\n",
    "\n",
    "        # Cross-Pair Memory\n",
    "        self.cross_pair_memory = CrossPairMemory(\n",
    "            num_pairs=config.num_pairs,\n",
    "            pair_dim=config.hidden_dim,\n",
    "            macro_dim=config.input_dim // 2,\n",
    "            num_correlation_slots=config.num_correlation_slots,\n",
    "        )\n",
    "\n",
    "        # Continuum Memory System\n",
    "        self.continuum_memory = ContinuumMemorySystem(\n",
    "            input_dim=config.hidden_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_levels=config.num_cms_levels,\n",
    "            base_frequency=config.cms_base_frequency,\n",
    "            frequency_multiplier=config.cms_frequency_multiplier,\n",
    "            hidden_dims=config.cms_hidden_dims,\n",
    "        )\n",
    "\n",
    "        # Session-aware Frequency Gate\n",
    "        self.session_gate = SessionFrequencyGate(\n",
    "            num_memory_levels=config.num_cms_levels,\n",
    "            session_embedding_dim=config.session_embedding_dim,\n",
    "        )\n",
    "\n",
    "        # Regime Detector\n",
    "        self.regime_detector = RegimeDetector(\n",
    "            input_dim=config.hidden_dim,\n",
    "            hidden_dim=config.regime_hidden_dim,\n",
    "            num_regimes=config.num_regimes,\n",
    "        )\n",
    "\n",
    "        # Final fusion: combine all pair outputs\n",
    "        self.pair_fusion = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim * config.num_pairs, config.hidden_dim),\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Output heads\n",
    "        self.output_heads = OutputHeads(\n",
    "            input_dim=config.hidden_dim,\n",
    "            num_direction_classes=config.num_direction_classes,\n",
    "            predict_volatility=config.predict_volatility,\n",
    "            predict_regime=config.predict_regime,\n",
    "            output_confidence=config.output_confidence,\n",
    "            num_regimes=config.num_regimes,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ohlc: torch.Tensor,\n",
    "        volume: Optional[torch.Tensor],\n",
    "        timestamps: torch.Tensor,\n",
    "        macro_data: Optional[Dict] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through NEXUS-FX.\n",
    "\n",
    "        Args:\n",
    "            ohlc: Multi-pair, multi-timeframe OHLC (batch, pairs, timeframes, seq, 4)\n",
    "            volume: Volume data (batch, pairs, timeframes, seq)\n",
    "            timestamps: Unix timestamps (batch, seq)\n",
    "            macro_data: Optional macro feature data dict\n",
    "\n",
    "        Returns:\n",
    "            outputs: Dictionary with all predictions\n",
    "        \"\"\"\n",
    "        batch_size, num_pairs, num_tf, seq_len, _ = ohlc.shape\n",
    "\n",
    "        # 1. Feature Engineering per pair and timeframe\n",
    "        pair_features = []\n",
    "        for p in range(num_pairs):\n",
    "            tf_features = []\n",
    "            for tf in range(num_tf):\n",
    "                ohlc_tf = ohlc[:, p, tf, :, :]  # (batch, seq, 4)\n",
    "                vol_tf = volume[:, p, tf, :] if volume is not None else None\n",
    "\n",
    "                features = self.feature_engine.compute_features(ohlc_tf, vol_tf)\n",
    "                tf_features.append(features)\n",
    "\n",
    "            # Concatenate timeframe features\n",
    "            pair_feat = torch.cat(tf_features, dim=-1)  # (batch, seq, features)\n",
    "            pair_features.append(pair_feat)\n",
    "\n",
    "        # Stack pairs: (batch, num_pairs, seq, features)\n",
    "        pair_features = torch.stack(pair_features, dim=1)\n",
    "\n",
    "        # 2. Project features to input_dim\n",
    "        pair_features = pair_features.view(batch_size * num_pairs, seq_len, -1)\n",
    "        pair_features = self.input_projection(pair_features)\n",
    "        pair_features = pair_features.view(batch_size, num_pairs, seq_len, -1)\n",
    "\n",
    "        # 3. Macro feature encoding\n",
    "        macro_features = self.macro_encoder.encode(\n",
    "            timestamps=timestamps,\n",
    "            calendar_data=macro_data.get('calendar') if macro_data else None,\n",
    "            rates_data=macro_data.get('rates') if macro_data else None,\n",
    "            yields_data=macro_data.get('yields') if macro_data else None,\n",
    "            commodities_data=macro_data.get('commodities') if macro_data else None,\n",
    "            sentiment_data=macro_data.get('sentiment') if macro_data else None,\n",
    "        )\n",
    "        macro_features = self.macro_projection(macro_features)  # (batch, seq, dim)\n",
    "\n",
    "        # 4. Session detection\n",
    "        session_features = self.session_clock.compute_session_features(timestamps)\n",
    "        session_emb = self.session_projection(session_features)  # (batch, seq, dim)\n",
    "\n",
    "        # Get session indicators for gating\n",
    "        session_indicators = self.session_clock.detect_sessions(timestamps)\n",
    "\n",
    "        # 5. Process each pair through Self-Modifying Titans\n",
    "        pair_states = []\n",
    "        for p in range(num_pairs):\n",
    "            # Combine pair features with macro\n",
    "            pair_input = torch.cat([\n",
    "                pair_features[:, p, :, :],\n",
    "                macro_features,\n",
    "            ], dim=-1)  # (batch, seq, input_dim + macro_dim)\n",
    "\n",
    "            # Process through Titans\n",
    "            titans_out, aux_info = self.titans_per_pair[p](pair_input)\n",
    "\n",
    "            # Take last timestep\n",
    "            pair_state = titans_out[:, -1, :]  # (batch, hidden_dim)\n",
    "            pair_states.append(pair_state)\n",
    "\n",
    "        # Stack: (batch, num_pairs, hidden_dim)\n",
    "        pair_states = torch.stack(pair_states, dim=1)\n",
    "\n",
    "        # 6. Cross-Pair Memory (learn correlations)\n",
    "        macro_state = macro_features[:, -1, :]  # Last timestep\n",
    "        enriched_states = self.cross_pair_memory(pair_states, macro_state)\n",
    "\n",
    "        # 7. Process through Continuum Memory System\n",
    "        # Average across pairs for CMS input\n",
    "        cms_input = enriched_states.mean(dim=1)  # (batch, hidden_dim)\n",
    "\n",
    "        cms_output, level_outputs = self.continuum_memory(\n",
    "            cms_input,\n",
    "            return_all_levels=True\n",
    "        )\n",
    "\n",
    "        # 8. Session-aware Frequency Gating\n",
    "        # Use last timestep's session indicators\n",
    "        session_ind_last = session_indicators[:, -1, :]  # (batch, 6)\n",
    "        gates = self.session_gate(session_ind_last)  # (batch, num_levels)\n",
    "\n",
    "        # Apply gates to CMS levels\n",
    "        gated_cms = self.session_gate.apply_gates(level_outputs, gates)\n",
    "\n",
    "        # 9. Regime Detection from slowest CMS level\n",
    "        slowest_level = level_outputs[-1]  # Slowest (macro) level\n",
    "        regime_logits, regime_features = self.regime_detector(slowest_level)\n",
    "\n",
    "        # 10. Fuse all pair states\n",
    "        pair_states_flat = enriched_states.view(batch_size, -1)\n",
    "        fused = self.pair_fusion(pair_states_flat)\n",
    "\n",
    "        # Combine with CMS and regime features\n",
    "        final_features = fused + gated_cms + regime_features\n",
    "\n",
    "        # 11. Output heads\n",
    "        outputs = self.output_heads(final_features)\n",
    "\n",
    "        # Add regime prediction\n",
    "        outputs['regime_logits'] = regime_logits\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def reset_memories(self) -> None:\n",
    "        \"\"\"Reset all memories (useful for online learning)\"\"\"\n",
    "        for titans in self.titans_per_pair:\n",
    "            titans.reset_memories()\n",
    "\n",
    "        self.cross_pair_memory.reset()\n",
    "        self.continuum_memory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bae8dad6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "bae8dad6",
    "outputId": "4107fa3f-f016-4cb0-f5df-e9ffc811a7df"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nDelta Gradient Descent (DGD) - L2-regression based optimizer.\\n\\nReplaces standard dot-product weight update with L2-regression objective,\\nresulting in adaptive decay based on current data distribution.\\n\\nTheoretical Background:\\n    In NSAM, parameter updates are framed as L2-regression problems rather\\n    than gradient descent. This creates adaptive learning rates and decay\\n    factors that respond to the current data distribution.\\n    \\n    Standard SGD: w \u2190 w - lr * grad\\n    \\n    DGD: w \u2190 w - lr * (grad + \u03bb * (w - w_ref))\\n    \\n    where \u03bb (decay) adapts based on mini-batch statistics, creating\\n    automatic regularization that strengthens when data is sparse/noisy\\n    and weakens when data is informative.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# ============= OPTIMIZER - DELTA GRADIENT DESCENT =============\n",
    "\"\"\"\n",
    "Delta Gradient Descent (DGD) - L2-regression based optimizer.\n",
    "\n",
    "Replaces standard dot-product weight update with L2-regression objective,\n",
    "resulting in adaptive decay based on current data distribution.\n",
    "\n",
    "Theoretical Background:\n",
    "    In NSAM, parameter updates are framed as L2-regression problems rather\n",
    "    than gradient descent. This creates adaptive learning rates and decay\n",
    "    factors that respond to the current data distribution.\n",
    "\n",
    "    Standard SGD: w \u2190 w - lr * grad\n",
    "\n",
    "    DGD: w \u2190 w - lr * (grad + \u03bb * (w - w_ref))\n",
    "\n",
    "    where \u03bb (decay) adapts based on mini-batch statistics, creating\n",
    "    automatic regularization that strengthens when data is sparse/noisy\n",
    "    and weakens when data is informative.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "318a467d",
   "metadata": {
    "id": "318a467d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7a1a6e9",
   "metadata": {
    "id": "c7a1a6e9"
   },
   "outputs": [],
   "source": [
    "class DeltaGradientDescent(Optimizer):\n",
    "    \"\"\"\n",
    "    Delta Gradient Descent optimizer.\n",
    "\n",
    "    Implements adaptive decay based on gradient statistics.\n",
    "\n",
    "    Args:\n",
    "        params: Model parameters\n",
    "        lr: Learning rate (default: 1e-3)\n",
    "        base_decay: Base decay factor (default: 0.01)\n",
    "        adaptive_decay: Whether to adapt decay based on gradients (default: True)\n",
    "        decay_momentum: Momentum for decay adaptation (default: 0.9)\n",
    "        eps: Small constant for numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        base_decay: float = 0.01,\n",
    "        adaptive_decay: bool = True,\n",
    "        decay_momentum: float = 0.9,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if base_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid base_decay: {base_decay}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            base_decay=base_decay,\n",
    "            adaptive_decay=adaptive_decay,\n",
    "            decay_momentum=decay_momentum,\n",
    "            eps=eps,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure: A closure that reevaluates the model and returns the loss\n",
    "\n",
    "        Returns:\n",
    "            loss (optional)\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            base_decay = group['base_decay']\n",
    "            adaptive_decay = group['adaptive_decay']\n",
    "            decay_momentum = group['decay_momentum']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "\n",
    "                # Initialize state\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['reference_param'] = p.data.clone()\n",
    "                    state['grad_variance'] = torch.zeros_like(p.data)\n",
    "                    state['adaptive_lambda'] = torch.ones_like(p.data) * base_decay\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Compute adaptive decay (\u03bb)\n",
    "                if adaptive_decay:\n",
    "                    # Update gradient variance estimate (exponential moving average)\n",
    "                    grad_sq = grad ** 2\n",
    "                    state['grad_variance'] = (\n",
    "                        decay_momentum * state['grad_variance'] +\n",
    "                        (1 - decay_momentum) * grad_sq\n",
    "                    )\n",
    "\n",
    "                    # Adapt lambda based on gradient variance\n",
    "                    # High variance \u2192 higher decay (more regularization)\n",
    "                    # Low variance \u2192 lower decay (trust the gradient)\n",
    "                    grad_std = torch.sqrt(state['grad_variance'] + eps)\n",
    "                    lambda_adaptive = base_decay * (1 + grad_std)\n",
    "\n",
    "                    # Smooth lambda updates\n",
    "                    state['adaptive_lambda'] = (\n",
    "                        decay_momentum * state['adaptive_lambda'] +\n",
    "                        (1 - decay_momentum) * lambda_adaptive\n",
    "                    )\n",
    "\n",
    "                    lambda_t = state['adaptive_lambda']\n",
    "                else:\n",
    "                    lambda_t = base_decay\n",
    "\n",
    "                # DGD update: w \u2190 w - lr * (grad + \u03bb * (w - w_ref))\n",
    "                # This is equivalent to L2 regression with reference point\n",
    "                decay_term = lambda_t * (p.data - state['reference_param'])\n",
    "                p.data.add_(grad + decay_term, alpha=-lr)\n",
    "\n",
    "                # Periodically update reference (acts as slow-moving target)\n",
    "                if state['step'] % 1000 == 0:\n",
    "                    state['reference_param'] = p.data.clone()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_decay_stats(self) -> dict:\n",
    "        \"\"\"Get statistics about adaptive decay factors\"\"\"\n",
    "        stats = {\n",
    "            'mean_lambda': [],\n",
    "            'std_lambda': [],\n",
    "            'max_lambda': [],\n",
    "        }\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state.get(p, {})\n",
    "                if 'adaptive_lambda' in state:\n",
    "                    lambda_t = state['adaptive_lambda']\n",
    "                    stats['mean_lambda'].append(lambda_t.mean().item())\n",
    "                    stats['std_lambda'].append(lambda_t.std().item())\n",
    "                    stats['max_lambda'].append(lambda_t.max().item())\n",
    "\n",
    "        # Average across all parameters\n",
    "        for key in stats:\n",
    "            if stats[key]:\n",
    "                stats[key] = sum(stats[key]) / len(stats[key])\n",
    "            else:\n",
    "                stats[key] = 0.0\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae7892ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "ae7892ae",
    "outputId": "3203b641-8b80-4593-9e5d-61b1970ff930"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMulti-Scale Momentum Muon (M3) - Multi-timescale momentum optimizer.\\n\\nMaintains momentum at multiple timescales, inspired by the nested memory\\nconcept in NSAM. Combines short-term and long-term momentum adaptively.\\n\\nTheoretical Background:\\n    Just as the model has memories at different timescales, the optimizer\\n    should have momentum at different timescales:\\n    \\n    - Short-term momentum: Captures recent gradient direction (fast adaptation)\\n    - Long-term momentum: Captures persistent optimization landscape (stability)\\n    \\n    The combination is learned during training, allowing the optimizer to\\n    balance rapid adaptation with stable convergence.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# ============= OPTIMIZER - MULTI-SCALE MOMENTUM =============\n",
    "\"\"\"\n",
    "Multi-Scale Momentum Muon (M3) - Multi-timescale momentum optimizer.\n",
    "\n",
    "Maintains momentum at multiple timescales, inspired by the nested memory\n",
    "concept in NSAM. Combines short-term and long-term momentum adaptively.\n",
    "\n",
    "Theoretical Background:\n",
    "    Just as the model has memories at different timescales, the optimizer\n",
    "    should have momentum at different timescales:\n",
    "\n",
    "    - Short-term momentum: Captures recent gradient direction (fast adaptation)\n",
    "    - Long-term momentum: Captures persistent optimization landscape (stability)\n",
    "\n",
    "    The combination is learned during training, allowing the optimizer to\n",
    "    balance rapid adaptation with stable convergence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62f3c909",
   "metadata": {
    "id": "62f3c909"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb26345d",
   "metadata": {
    "id": "cb26345d"
   },
   "outputs": [],
   "source": [
    "class MultiScaleMomentumMuon(Optimizer):\n",
    "    \"\"\"\n",
    "    Multi-Scale Momentum Muon optimizer.\n",
    "\n",
    "    Maintains momentum buffers at multiple timescales and combines them\n",
    "    via learned mixing weights.\n",
    "\n",
    "    Args:\n",
    "        params: Model parameters\n",
    "        lr: Learning rate (default: 1e-3)\n",
    "        betas: Tuple of (short_momentum, long_momentum) (default: (0.9, 0.999))\n",
    "        weight_decay: Weight decay factor (default: 0.01)\n",
    "        eps: Small constant for numerical stability (default: 1e-8)\n",
    "        adaptive_mixing: Whether to adapt momentum mixing (default: True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: tuple = (0.9, 0.999),\n",
    "        weight_decay: float = 0.01,\n",
    "        eps: float = 1e-8,\n",
    "        adaptive_mixing: bool = True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta1: {betas[1]}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            weight_decay=weight_decay,\n",
    "            eps=eps,\n",
    "            adaptive_mixing=adaptive_mixing,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure: A closure that reevaluates the model and returns the loss\n",
    "\n",
    "        Returns:\n",
    "            loss (optional)\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta_short, beta_long = group['betas']\n",
    "            weight_decay = group['weight_decay']\n",
    "            eps = group['eps']\n",
    "            adaptive_mixing = group['adaptive_mixing']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "\n",
    "                # Apply weight decay\n",
    "                if weight_decay != 0:\n",
    "                    grad = grad.add(p.data, alpha=weight_decay)\n",
    "\n",
    "                # Initialize state\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['momentum_short'] = torch.zeros_like(p.data)\n",
    "                    state['momentum_long'] = torch.zeros_like(p.data)\n",
    "                    state['mixing_weight'] = 0.5  # Start with equal mixing\n",
    "                    state['grad_variance_short'] = torch.zeros_like(p.data)\n",
    "                    state['grad_variance_long'] = torch.zeros_like(p.data)\n",
    "\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "\n",
    "                # Update short-term momentum\n",
    "                state['momentum_short'] = (\n",
    "                    beta_short * state['momentum_short'] +\n",
    "                    (1 - beta_short) * grad\n",
    "                )\n",
    "\n",
    "                # Update long-term momentum\n",
    "                state['momentum_long'] = (\n",
    "                    beta_long * state['momentum_long'] +\n",
    "                    (1 - beta_long) * grad\n",
    "                )\n",
    "\n",
    "                # Bias correction\n",
    "                momentum_short_hat = state['momentum_short'] / (1 - beta_short ** t)\n",
    "                momentum_long_hat = state['momentum_long'] / (1 - beta_long ** t)\n",
    "\n",
    "                # Adaptive mixing\n",
    "                if adaptive_mixing:\n",
    "                    # Update gradient variance estimates\n",
    "                    grad_sq = grad ** 2\n",
    "                    state['grad_variance_short'] = (\n",
    "                        beta_short * state['grad_variance_short'] +\n",
    "                        (1 - beta_short) * grad_sq\n",
    "                    )\n",
    "                    state['grad_variance_long'] = (\n",
    "                        beta_long * state['grad_variance_long'] +\n",
    "                        (1 - beta_long) * grad_sq\n",
    "                    )\n",
    "\n",
    "                    # Adapt mixing based on gradient variance\n",
    "                    # If short-term variance is high, rely more on long-term\n",
    "                    var_short = state['grad_variance_short'] / (1 - beta_short ** t)\n",
    "                    var_long = state['grad_variance_long'] / (1 - beta_long ** t)\n",
    "\n",
    "                    # Mixing weight: 0 = all short, 1 = all long\n",
    "                    var_ratio = var_short / (var_long + eps)\n",
    "                    mixing_raw = torch.sigmoid(torch.log(var_ratio + eps))\n",
    "\n",
    "                    # Smooth mixing weight updates\n",
    "                    state['mixing_weight'] = (\n",
    "                        0.9 * state['mixing_weight'] +\n",
    "                        0.1 * mixing_raw.mean().item()\n",
    "                    )\n",
    "\n",
    "                # Combine momenta\n",
    "                alpha = state['mixing_weight']\n",
    "                combined_momentum = (\n",
    "                    (1 - alpha) * momentum_short_hat +\n",
    "                    alpha * momentum_long_hat\n",
    "                )\n",
    "\n",
    "                # Update parameters\n",
    "                p.data.add_(combined_momentum, alpha=-lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_momentum_stats(self) -> dict:\n",
    "        \"\"\"Get statistics about momentum mixing\"\"\"\n",
    "        stats = {\n",
    "            'mean_mixing_weight': [],\n",
    "            'momentum_short_norm': [],\n",
    "            'momentum_long_norm': [],\n",
    "        }\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state.get(p, {})\n",
    "                if 'mixing_weight' in state:\n",
    "                    stats['mean_mixing_weight'].append(state['mixing_weight'])\n",
    "                if 'momentum_short' in state:\n",
    "                    stats['momentum_short_norm'].append(\n",
    "                        state['momentum_short'].norm().item()\n",
    "                    )\n",
    "                if 'momentum_long' in state:\n",
    "                    stats['momentum_long_norm'].append(\n",
    "                        state['momentum_long'].norm().item()\n",
    "                    )\n",
    "\n",
    "        # Average across all parameters\n",
    "        for key in stats:\n",
    "            if stats[key]:\n",
    "                stats[key] = sum(stats[key]) / len(stats[key])\n",
    "            else:\n",
    "                stats[key] = 0.0\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c7f61df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "6c7f61df",
    "outputId": "9aacf88a-be11-4f25-bc65-a0ef6021b06c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMulti-objective Loss Functions for NEXUS-FX.\\n\\nCombines multiple prediction objectives:\\n1. Direction prediction (classification)\\n2. Volatility prediction (regression)\\n3. Regime prediction (classification)\\n4. Confidence calibration (alignment of predicted confidence with accuracy)\\n\\nEach objective has an adaptive weight that adjusts during training.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# ============= TRAINING - LOSSES =============\n",
    "\"\"\"\n",
    "Multi-objective Loss Functions for NEXUS-FX.\n",
    "\n",
    "Combines multiple prediction objectives:\n",
    "1. Direction prediction (classification)\n",
    "2. Volatility prediction (regression)\n",
    "3. Regime prediction (classification)\n",
    "4. Confidence calibration (alignment of predicted confidence with accuracy)\n",
    "\n",
    "Each objective has an adaptive weight that adjusts during training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "805e8a2b",
   "metadata": {
    "id": "805e8a2b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96d00738",
   "metadata": {
    "id": "96d00738"
   },
   "outputs": [],
   "source": [
    "class NexusFXLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-objective loss for NEXUS-FX model.\n",
    "\n",
    "    Combines:\n",
    "    - Direction loss (cross-entropy)\n",
    "    - Volatility loss (MSE)\n",
    "    - Regime loss (cross-entropy)\n",
    "    - Calibration loss (confidence vs accuracy alignment)\n",
    "\n",
    "    Args:\n",
    "        direction_weight: Weight for direction loss\n",
    "        volatility_weight: Weight for volatility loss\n",
    "        regime_weight: Weight for regime loss\n",
    "        calibration_weight: Weight for calibration loss\n",
    "        adaptive_weights: Whether to adapt weights during training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        direction_weight: float = 1.0,\n",
    "        volatility_weight: float = 0.5,\n",
    "        regime_weight: float = 0.3,\n",
    "        calibration_weight: float = 0.2,\n",
    "        adaptive_weights: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('direction_weight', torch.tensor(direction_weight))\n",
    "        self.register_buffer('volatility_weight', torch.tensor(volatility_weight))\n",
    "        self.register_buffer('regime_weight', torch.tensor(regime_weight))\n",
    "        self.register_buffer('calibration_weight', torch.tensor(calibration_weight))\n",
    "\n",
    "        self.adaptive_weights = adaptive_weights\n",
    "\n",
    "        # Track loss magnitudes for adaptive weighting\n",
    "        self.register_buffer('direction_ema', torch.tensor(1.0))\n",
    "        self.register_buffer('volatility_ema', torch.tensor(1.0))\n",
    "        self.register_buffer('regime_ema', torch.tensor(1.0))\n",
    "        self.register_buffer('calibration_ema', torch.tensor(1.0))\n",
    "\n",
    "        self.ema_momentum = 0.9\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        outputs: Dict[str, torch.Tensor],\n",
    "        targets: Dict[str, torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute multi-objective loss.\n",
    "\n",
    "        Args:\n",
    "            outputs: Model outputs dict with keys:\n",
    "                - direction_logits: (batch, 3)\n",
    "                - volatility: (batch, 1)\n",
    "                - regime_logits: (batch, num_regimes)\n",
    "                - confidence: (batch, 1)\n",
    "            targets: Target dict with keys:\n",
    "                - direction: (batch,) with values in {0, 1, 2}\n",
    "                - volatility: (batch,) target volatility\n",
    "                - regime: (batch,) regime labels (optional)\n",
    "\n",
    "        Returns:\n",
    "            total_loss: Combined weighted loss\n",
    "            loss_dict: Individual losses for logging\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "\n",
    "        # 1. Direction loss (cross-entropy)\n",
    "        if 'direction_logits' in outputs and 'direction' in targets:\n",
    "            direction_loss = F.cross_entropy(\n",
    "                outputs['direction_logits'],\n",
    "                targets['direction'].long()\n",
    "            )\n",
    "            losses['direction'] = direction_loss\n",
    "\n",
    "        # 2. Volatility loss (MSE)\n",
    "        if 'volatility' in outputs and 'volatility' in targets:\n",
    "            volatility_loss = F.mse_loss(\n",
    "                outputs['volatility'].squeeze(-1),\n",
    "                targets['volatility']\n",
    "            )\n",
    "            losses['volatility'] = volatility_loss\n",
    "\n",
    "        # 3. Regime loss (cross-entropy)\n",
    "        if 'regime_logits' in outputs and 'regime' in targets:\n",
    "            regime_loss = F.cross_entropy(\n",
    "                outputs['regime_logits'],\n",
    "                targets['regime'].long()\n",
    "            )\n",
    "            losses['regime'] = regime_loss\n",
    "\n",
    "        # 4. Calibration loss\n",
    "        if 'confidence' in outputs and 'direction_logits' in outputs and 'direction' in targets:\n",
    "            calibration_loss = self._compute_calibration_loss(\n",
    "                outputs['direction_logits'],\n",
    "                outputs['confidence'],\n",
    "                targets['direction']\n",
    "            )\n",
    "            losses['calibration'] = calibration_loss\n",
    "\n",
    "        # Update EMAs\n",
    "        if self.training and self.adaptive_weights:\n",
    "            self._update_loss_emas(losses)\n",
    "\n",
    "        # Compute weighted sum\n",
    "        total_loss = torch.tensor(0.0, device=next(iter(losses.values())).device)\n",
    "\n",
    "        if 'direction' in losses:\n",
    "            weight = self._get_adaptive_weight('direction')\n",
    "            total_loss = total_loss + weight * losses['direction']\n",
    "\n",
    "        if 'volatility' in losses:\n",
    "            weight = self._get_adaptive_weight('volatility')\n",
    "            total_loss = total_loss + weight * losses['volatility']\n",
    "\n",
    "        if 'regime' in losses:\n",
    "            weight = self._get_adaptive_weight('regime')\n",
    "            total_loss = total_loss + weight * losses['regime']\n",
    "\n",
    "        if 'calibration' in losses:\n",
    "            weight = self._get_adaptive_weight('calibration')\n",
    "            total_loss = total_loss + weight * losses['calibration']\n",
    "\n",
    "        # Add total to dict\n",
    "        losses['total'] = total_loss\n",
    "\n",
    "        return total_loss, losses\n",
    "\n",
    "    def _compute_calibration_loss(\n",
    "        self,\n",
    "        direction_logits: torch.Tensor,\n",
    "        confidence: torch.Tensor,\n",
    "        direction_targets: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calibration loss: predicted confidence should match actual accuracy.\n",
    "\n",
    "        High confidence predictions should be more accurate than low confidence ones.\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        pred_classes = torch.argmax(direction_logits, dim=-1)\n",
    "        correct = (pred_classes == direction_targets.long()).float()\n",
    "\n",
    "        # Confidence should match correctness\n",
    "        # If model is 90% confident, it should be right 90% of the time\n",
    "        calibration_loss = F.mse_loss(confidence.squeeze(-1), correct)\n",
    "\n",
    "        return calibration_loss\n",
    "\n",
    "    def _update_loss_emas(self, losses: Dict[str, torch.Tensor]) -> None:\n",
    "        \"\"\"Update exponential moving averages of loss magnitudes\"\"\"\n",
    "        if 'direction' in losses:\n",
    "            self.direction_ema = (\n",
    "                self.ema_momentum * self.direction_ema +\n",
    "                (1 - self.ema_momentum) * losses['direction'].detach()\n",
    "            )\n",
    "\n",
    "        if 'volatility' in losses:\n",
    "            self.volatility_ema = (\n",
    "                self.ema_momentum * self.volatility_ema +\n",
    "                (1 - self.ema_momentum) * losses['volatility'].detach()\n",
    "            )\n",
    "\n",
    "        if 'regime' in losses:\n",
    "            self.regime_ema = (\n",
    "                self.ema_momentum * self.regime_ema +\n",
    "                (1 - self.ema_momentum) * losses['regime'].detach()\n",
    "            )\n",
    "\n",
    "        if 'calibration' in losses:\n",
    "            self.calibration_ema = (\n",
    "                self.ema_momentum * self.calibration_ema +\n",
    "                (1 - self.ema_momentum) * losses['calibration'].detach()\n",
    "            )\n",
    "\n",
    "    def _get_adaptive_weight(self, loss_name: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get adaptive weight for a loss component.\n",
    "\n",
    "        Balances losses by normalizing by their typical magnitude.\n",
    "        \"\"\"\n",
    "        if not self.adaptive_weights:\n",
    "            if loss_name == 'direction':\n",
    "                return self.direction_weight\n",
    "            elif loss_name == 'volatility':\n",
    "                return self.volatility_weight\n",
    "            elif loss_name == 'regime':\n",
    "                return self.regime_weight\n",
    "            elif loss_name == 'calibration':\n",
    "                return self.calibration_weight\n",
    "\n",
    "        # Adaptive weighting based on EMA magnitudes\n",
    "        # Normalize so all losses contribute roughly equally\n",
    "        total_ema = (\n",
    "            self.direction_ema + self.volatility_ema +\n",
    "            self.regime_ema + self.calibration_ema\n",
    "        )\n",
    "\n",
    "        if loss_name == 'direction':\n",
    "            base_weight = self.direction_weight\n",
    "            adaptive_factor = total_ema / (self.direction_ema + 1e-8)\n",
    "        elif loss_name == 'volatility':\n",
    "            base_weight = self.volatility_weight\n",
    "            adaptive_factor = total_ema / (self.volatility_ema + 1e-8)\n",
    "        elif loss_name == 'regime':\n",
    "            base_weight = self.regime_weight\n",
    "            adaptive_factor = total_ema / (self.regime_ema + 1e-8)\n",
    "        elif loss_name == 'calibration':\n",
    "            base_weight = self.calibration_weight\n",
    "            adaptive_factor = total_ema / (self.calibration_ema + 1e-8)\n",
    "        else:\n",
    "            return torch.tensor(1.0)\n",
    "\n",
    "        return base_weight * adaptive_factor / 4  # Normalize by number of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ca5a06c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "6ca5a06c",
    "outputId": "26aeda71-c411-4cbc-d87c-3267c1087893"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nForex-specific evaluation metrics.\\n\\nMetrics include:\\n- Sharpe ratio\\n- Sortino ratio\\n- Maximum drawdown\\n- Win rate\\n- Profit factor\\n- Per-regime performance\\n- Calibration metrics\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# ============= TRAINING - EVALUATION =============\n",
    "\"\"\"\n",
    "Forex-specific evaluation metrics.\n",
    "\n",
    "Metrics include:\n",
    "- Sharpe ratio\n",
    "- Sortino ratio\n",
    "- Maximum drawdown\n",
    "- Win rate\n",
    "- Profit factor\n",
    "- Per-regime performance\n",
    "- Calibration metrics\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "724bd50d",
   "metadata": {
    "id": "724bd50d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b62289a1",
   "metadata": {
    "id": "b62289a1"
   },
   "outputs": [],
   "source": [
    "class NexusFXEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for forex trading performance.\n",
    "\n",
    "    Computes both prediction accuracy metrics and trading performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset accumulated statistics\"\"\"\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.confidences = []\n",
    "        self.returns = []\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        predictions: Dict[str, torch.Tensor],\n",
    "        targets: Dict[str, torch.Tensor],\n",
    "        returns: Optional[torch.Tensor] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Update with new batch of predictions.\n",
    "\n",
    "        Args:\n",
    "            predictions: Model predictions dict\n",
    "            targets: Ground truth targets dict\n",
    "            returns: Actual returns (optional, for trading metrics)\n",
    "        \"\"\"\n",
    "        self.predictions.append(predictions)\n",
    "        self.targets.append(targets)\n",
    "\n",
    "        if 'confidence' in predictions:\n",
    "            self.confidences.append(predictions['confidence'].cpu())\n",
    "\n",
    "        if returns is not None:\n",
    "            self.returns.append(returns.cpu())\n",
    "\n",
    "    def compute_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute all evaluation metrics.\n",
    "\n",
    "        Returns:\n",
    "            metrics: Dictionary of metric name -> value\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Prediction accuracy metrics\n",
    "        acc_metrics = self._compute_accuracy_metrics()\n",
    "        metrics.update(acc_metrics)\n",
    "\n",
    "        # Trading performance metrics\n",
    "        if len(self.returns) > 0:\n",
    "            trading_metrics = self._compute_trading_metrics()\n",
    "            metrics.update(trading_metrics)\n",
    "\n",
    "        # Calibration metrics\n",
    "        if len(self.confidences) > 0:\n",
    "            cal_metrics = self._compute_calibration_metrics()\n",
    "            metrics.update(cal_metrics)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _compute_accuracy_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute prediction accuracy metrics\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Direction accuracy\n",
    "        all_pred_classes = []\n",
    "        all_target_classes = []\n",
    "\n",
    "        for pred, target in zip(self.predictions, self.targets):\n",
    "            if 'direction_logits' in pred and 'direction' in target:\n",
    "                pred_class = torch.argmax(pred['direction_logits'], dim=-1)\n",
    "                all_pred_classes.append(pred_class.cpu())\n",
    "                all_target_classes.append(target['direction'].cpu())\n",
    "\n",
    "        if all_pred_classes:\n",
    "            pred_classes = torch.cat(all_pred_classes)\n",
    "            target_classes = torch.cat(all_target_classes)\n",
    "\n",
    "            accuracy = (pred_classes == target_classes).float().mean().item()\n",
    "            metrics['direction_accuracy'] = accuracy\n",
    "\n",
    "            # Per-class accuracy\n",
    "            for i in range(3):  # Assuming 3 classes\n",
    "                mask = target_classes == i\n",
    "                if mask.sum() > 0:\n",
    "                    class_acc = (pred_classes[mask] == target_classes[mask]).float().mean().item()\n",
    "                    metrics[f'direction_accuracy_class_{i}'] = class_acc\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _compute_trading_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute trading performance metrics\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Concatenate all returns\n",
    "        all_returns = torch.cat(self.returns).numpy()\n",
    "\n",
    "        # Cumulative returns\n",
    "        cumulative_returns = np.cumprod(1 + all_returns) - 1\n",
    "        total_return = cumulative_returns[-1]\n",
    "        metrics['total_return'] = total_return\n",
    "\n",
    "        # Sharpe ratio (annualized, assuming 5-min returns)\n",
    "        # 252 trading days * 24 hours * 12 (5-min periods per hour)\n",
    "        periods_per_year = 252 * 24 * 12\n",
    "        sharpe = np.mean(all_returns) / (np.std(all_returns) + 1e-8) * np.sqrt(periods_per_year)\n",
    "        metrics['sharpe_ratio'] = sharpe\n",
    "\n",
    "        # Sortino ratio (only downside volatility)\n",
    "        downside_returns = all_returns[all_returns < 0]\n",
    "        if len(downside_returns) > 0:\n",
    "            sortino = np.mean(all_returns) / (np.std(downside_returns) + 1e-8) * np.sqrt(periods_per_year)\n",
    "            metrics['sortino_ratio'] = sortino\n",
    "\n",
    "        # Maximum drawdown\n",
    "        cumulative = np.cumprod(1 + all_returns)\n",
    "        running_max = np.maximum.accumulate(cumulative)\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        metrics['max_drawdown'] = max_drawdown\n",
    "\n",
    "        # Win rate\n",
    "        winning_trades = (all_returns > 0).sum()\n",
    "        total_trades = len(all_returns)\n",
    "        win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "        metrics['win_rate'] = win_rate\n",
    "\n",
    "        # Profit factor\n",
    "        gross_profit = all_returns[all_returns > 0].sum()\n",
    "        gross_loss = -all_returns[all_returns < 0].sum()\n",
    "        profit_factor = gross_profit / gross_loss if gross_loss > 0 else 0\n",
    "        metrics['profit_factor'] = profit_factor\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _compute_calibration_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute calibration metrics\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Confidence calibration\n",
    "        all_confidences = torch.cat(self.confidences)\n",
    "        all_pred_classes = []\n",
    "        all_target_classes = []\n",
    "\n",
    "        for pred, target in zip(self.predictions, self.targets):\n",
    "            if 'direction_logits' in pred and 'direction' in target:\n",
    "                pred_class = torch.argmax(pred['direction_logits'], dim=-1)\n",
    "                all_pred_classes.append(pred_class.cpu())\n",
    "                all_target_classes.append(target['direction'].cpu())\n",
    "\n",
    "        if all_pred_classes:\n",
    "            pred_classes = torch.cat(all_pred_classes)\n",
    "            target_classes = torch.cat(all_target_classes)\n",
    "            correct = (pred_classes == target_classes).float()\n",
    "\n",
    "            # Expected Calibration Error (ECE)\n",
    "            num_bins = 10\n",
    "            bin_boundaries = torch.linspace(0, 1, num_bins + 1)\n",
    "            ece = 0.0\n",
    "\n",
    "            for i in range(num_bins):\n",
    "                bin_lower = bin_boundaries[i]\n",
    "                bin_upper = bin_boundaries[i + 1]\n",
    "\n",
    "                in_bin = (all_confidences >= bin_lower) & (all_confidences < bin_upper)\n",
    "                in_bin = in_bin.squeeze()\n",
    "\n",
    "                if in_bin.sum() > 0:\n",
    "                    bin_confidence = all_confidences[in_bin].mean()\n",
    "                    bin_accuracy = correct[in_bin].mean()\n",
    "                    ece += torch.abs(bin_confidence - bin_accuracy) * (in_bin.sum() / len(all_confidences))\n",
    "\n",
    "            metrics['expected_calibration_error'] = ece.item()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def get_performance_summary(self) -> str:\n",
    "        \"\"\"Get formatted performance summary\"\"\"\n",
    "        metrics = self.compute_metrics()\n",
    "\n",
    "        summary = \"=== NEXUS-FX Performance Summary ===\\n\\n\"\n",
    "\n",
    "        summary += \"Prediction Metrics:\\n\"\n",
    "        summary += f\"  Direction Accuracy: {metrics.get('direction_accuracy', 0):.4f}\\n\"\n",
    "\n",
    "        summary += \"\\nTrading Metrics:\\n\"\n",
    "        summary += f\"  Total Return: {metrics.get('total_return', 0):.4f}\\n\"\n",
    "        summary += f\"  Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.4f}\\n\"\n",
    "        summary += f\"  Sortino Ratio: {metrics.get('sortino_ratio', 0):.4f}\\n\"\n",
    "        summary += f\"  Max Drawdown: {metrics.get('max_drawdown', 0):.4f}\\n\"\n",
    "        summary += f\"  Win Rate: {metrics.get('win_rate', 0):.4f}\\n\"\n",
    "        summary += f\"  Profit Factor: {metrics.get('profit_factor', 0):.4f}\\n\"\n",
    "\n",
    "        summary += \"\\nCalibration Metrics:\\n\"\n",
    "        summary += f\"  Expected Calibration Error: {metrics.get('expected_calibration_error', 0):.4f}\\n\"\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90909ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "90909ca7",
    "outputId": "270f84c8-7e24-42f1-d352-c7f4adecbb39"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nMain training loop for NEXUS-FX with continual learning support.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# ============= TRAINING - TRAINER =============\n",
    "\"\"\"\n",
    "Main training loop for NEXUS-FX with continual learning support.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c933709f",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "c933709f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12e7d6f3",
   "metadata": {
    "id": "12e7d6f3"
   },
   "outputs": [],
   "source": [
    "class NexusFXTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for NEXUS-FX model.\n",
    "\n",
    "    Supports:\n",
    "    - Continual learning\n",
    "    - Multi-scale memory updates\n",
    "    - Gradient clipping\n",
    "    - Learning rate scheduling\n",
    "    - Checkpointing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        config: NexusFXConfig,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: Optional[DataLoader] = None,\n",
    "        device: str = 'cuda',\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = NexusFXLoss(\n",
    "            direction_weight=config.direction_loss_weight,\n",
    "            volatility_weight=config.volatility_loss_weight,\n",
    "            regime_weight=config.regime_loss_weight,\n",
    "            calibration_weight=config.calibration_loss_weight,\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = self._create_optimizer()\n",
    "\n",
    "        # Evaluator\n",
    "        self.evaluator = NexusFXEvaluator()\n",
    "\n",
    "        # Training state\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        \"\"\"Create optimizer based on config\"\"\"\n",
    "        if self.config.optimizer_type == 'delta_gd':\n",
    "            return DeltaGradientDescent(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                base_decay=self.config.weight_decay,\n",
    "            )\n",
    "        elif self.config.optimizer_type == 'multi_scale_momentum':\n",
    "            return MultiScaleMomentumMuon(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "        else:  # Adam fallback\n",
    "            return torch.optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "            )\n",
    "\n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {self.current_epoch}\")\n",
    "        for batch in pbar:\n",
    "            loss, losses_dict = self.train_step(batch)\n",
    "            epoch_losses.append(losses_dict)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'step': self.global_step,\n",
    "            })\n",
    "\n",
    "        # Average losses\n",
    "        avg_losses = {}\n",
    "        for key in epoch_losses[0].keys():\n",
    "            # Ensure losses are on CPU before converting to item()\n",
    "            avg_losses[key] = sum(d[key].cpu().item() for d in epoch_losses) / len(epoch_losses)\n",
    "\n",
    "        return avg_losses\n",
    "\n",
    "    def train_step(self, batch: Dict[str, torch.Tensor]) -> tuple:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass through the actual model\n",
    "        outputs = self.model(\n",
    "            ohlc=batch['ohlc'],\n",
    "            volume=batch['volume'],\n",
    "            timestamps=batch['timestamps'],\n",
    "            macro_data=None # Assuming no macro data for this example\n",
    "        )\n",
    "\n",
    "        targets = {\n",
    "            'direction': batch['direction'],\n",
    "            'volatility': batch['volatility'],\n",
    "            'regime': batch['regime'],\n",
    "        }\n",
    "\n",
    "        # Compute loss\n",
    "        loss, losses_dict = self.criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        if self.config.gradient_clip_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.config.gradient_clip_norm\n",
    "            )\n",
    "\n",
    "        # Optimizer step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.global_step += 1\n",
    "\n",
    "        return loss, losses_dict\n",
    "\n",
    "    def validate(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Run validation\n",
    "        \"\"\"\n",
    "        if self.val_loader is None:\n",
    "            return {}\n",
    "\n",
    "        self.model.eval()\n",
    "        self.evaluator.reset()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "                        for k, v in batch.items()}\n",
    "\n",
    "                # Forward pass through the actual model\n",
    "                outputs = self.model(\n",
    "                    ohlc=batch['ohlc'],\n",
    "                    volume=batch['volume'],\n",
    "                    timestamps=batch['timestamps'],\n",
    "                    macro_data=None # Assuming no macro data for this example\n",
    "                )\n",
    "\n",
    "                targets = {\n",
    "                    'direction': batch['direction'],\n",
    "                    'volatility': batch['volatility'],\n",
    "                    'regime': batch['regime'],\n",
    "                }\n",
    "\n",
    "                loss, losses_dict = self.criterion(outputs, targets)\n",
    "                val_losses.append(losses_dict)\n",
    "\n",
    "                self.evaluator.update(outputs, targets)\n",
    "\n",
    "        # Average losses\n",
    "        avg_losses = {}\n",
    "        for key in val_losses[0].keys():\n",
    "             # Ensure losses are on CPU before converting to item()\n",
    "            avg_losses[key] = sum(d[key].cpu().item() for d in val_losses) / len(val_losses)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = self.evaluator.compute_metrics()\n",
    "        avg_losses.update(metrics)\n",
    "\n",
    "        return avg_losses\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            # Train epoch\n",
    "            train_losses = self.train_epoch()\n",
    "            print(f\"\\nEpoch {epoch} - Train Loss: {train_losses.get('total', 0):.4f}\")\n",
    "\n",
    "            # Validate\n",
    "            if self.val_loader is not None:\n",
    "                val_losses = self.validate()\n",
    "                print(f\"Epoch {epoch} - Val Loss: {val_losses.get('total', 0):.4f}\")\n",
    "\n",
    "                # Save best model\n",
    "                if val_losses.get('total', float('inf')) < self.best_val_loss:\n",
    "                    self.best_val_loss = val_losses['total']\n",
    "                    self.save_checkpoint('best_model.pt')\n",
    "\n",
    "            # Periodic checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.save_checkpoint(f'checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "    def save_checkpoint(self, filename: str) -> None:\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': self.current_epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config,\n",
    "        }\n",
    "\n",
    "        os.makedirs('checkpoints', exist_ok=True)\n",
    "        torch.save(checkpoint, os.path.join('checkpoints', filename))\n",
    "        print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "    def load_checkpoint(self, filename: str) -> None:\n",
    "        \"\"\"Load model checkpoint\"\"\"\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.global_step = checkpoint['global_step']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "        print(f\"Checkpoint loaded: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba81f5b",
   "metadata": {
    "id": "cba81f5b"
   },
   "source": [
    "============================================================================\n",
    "MODULE METADATA & EXPORTS\n",
    "============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a013573f",
   "metadata": {
    "id": "a013573f"
   },
   "outputs": [],
   "source": [
    "__version__ = \"1.0.0\"\n",
    "__author__ = \"NEXUS-FX Team\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d8a86",
   "metadata": {
    "id": "314d8a86"
   },
   "source": [
    "Version History:\n",
    "1.0.0 (2024-02-12): Initial consolidated release\n",
    "  - Combined all 35 modules from nexus_fx package\n",
    "  - 32 classes, 23 utility functions\n",
    "  - Complete NEXUS-FX functionality in single file\n",
    "  - Tested and verified for Colab/Kaggle usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3dc33f99",
   "metadata": {
    "id": "3dc33f99"
   },
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    # Configuration\n",
    "    'NexusFXConfig',\n",
    "\n",
    "    # Model Components\n",
    "    'NEXUSFX',\n",
    "    'AssociativeMemory',\n",
    "    'ContinuumMemorySystem',\n",
    "    'ContinuumMemoryLevel',\n",
    "    'CrossPairMemory',\n",
    "    'RegimeDetector',\n",
    "    'SessionFrequencyGate',\n",
    "    'SelfModifyingTitans',\n",
    "    'SelfModifyingTitansLayer',\n",
    "    'OutputHeads',\n",
    "\n",
    "    # Data\n",
    "    'ForexDataset',\n",
    "    'Preprocessor',\n",
    "    'FeatureEngine',\n",
    "    'MacroFeatureEncoder',\n",
    "    'SessionClock',\n",
    "\n",
    "    # Training\n",
    "    'NexusFXTrainer',\n",
    "    'NexusFXLoss',\n",
    "    'NexusFXEvaluator',\n",
    "\n",
    "    # Optimizers\n",
    "    'DeltaGradientDescent',\n",
    "    'MultiScaleMomentumMuon',\n",
    "\n",
    "    # Utilities\n",
    "    'setup_logger',\n",
    "    'MetricsLogger',\n",
    "    'get_active_sessions',\n",
    "    'is_market_open',\n",
    "    'calculate_spread',\n",
    "    'detect_session',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c32c23fb",
   "metadata": {
    "id": "c32c23fb"
   },
   "outputs": [],
   "source": [
    "def get_version():\n",
    "    \"\"\"Return the version of this module.\"\"\"\n",
    "    return __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e43a5159",
   "metadata": {
    "id": "e43a5159"
   },
   "outputs": [],
   "source": [
    "def list_components():\n",
    "    \"\"\"List all available components in the consolidated module.\"\"\"\n",
    "    components = {\n",
    "        'Models': ['NEXUSFX', 'AssociativeMemory', 'ContinuumMemorySystem',\n",
    "                   'CrossPairMemory', 'RegimeDetector', 'SessionFrequencyGate',\n",
    "                   'SelfModifyingTitans', 'OutputHeads'],\n",
    "        'Data': ['ForexDataset', 'Preprocessor', 'FeatureEngine',\n",
    "                 'MacroFeatureEncoder', 'SessionClock'],\n",
    "        'Training': ['NexusFXTrainer', 'NexusFXLoss', 'NexusFXEvaluator'],\n",
    "        'Optimizers': ['DeltaGradientDescent', 'MultiScaleMomentumMuon'],\n",
    "        'Utilities': ['setup_logger', 'MetricsLogger', 'get_active_sessions',\n",
    "                      'is_market_open', 'calculate_spread', 'detect_session'],\n",
    "    }\n",
    "    return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f30eff7",
   "metadata": {
    "id": "1f30eff7"
   },
   "outputs": [],
   "source": [
    "def quick_start_example():\n",
    "    \"\"\"\n",
    "    Return a quick start code example.\n",
    "    \"\"\"\n",
    "    example = \"\"\"\n",
    "    # Quick Start Example for NEXUS-FX\n",
    "\n",
    "    import nexus_fx_consolidated as nfx\n",
    "    import torch\n",
    "\n",
    "    # 1. Create configuration\n",
    "    config = nfx.NexusFXConfig(\n",
    "        pairs=['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD'],\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # 2. Create dataset\n",
    "    dataset = nfx.ForexDataset(\n",
    "        data_path='/path/to/forex/data',  # Or None for synthetic data\n",
    "        pairs=config.pairs,\n",
    "        base_timeframe='5m',\n",
    "        target_timeframes=config.timeframes,\n",
    "        sequence_length=config.sequence_length,\n",
    "    )\n",
    "\n",
    "    # 3. Create model\n",
    "    model = nfx.NEXUSFX(config)\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "    # 4. Create trainer\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    trainer = nfx.NexusFXTrainer(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        train_loader=train_loader,\n",
    "        device=config.device\n",
    "    )\n",
    "\n",
    "    # 5. Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 6. Evaluate\n",
    "    evaluator = nfx.NexusFXEvaluator()\n",
    "    # ... evaluation code ...\n",
    "    metrics = evaluator.compute_metrics()\n",
    "    print(metrics)\n",
    "    \"\"\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89635860",
   "metadata": {
    "lines_to_next_cell": 2,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "89635860",
    "outputId": "61b634d3-806d-4a78-af56-ac8144eb2ac0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NEXUS-FX v1.0.0 - Consolidated module loaded successfully\n",
      "Available components: 27 classes and functions\n",
      "Use help(nexus_fx_consolidated) for more information\n"
     ]
    }
   ],
   "source": [
    "# Print module info when imported\n",
    "if __name__ == '__main__':\n",
    "    print(f\"NEXUS-FX v{__version__} - Consolidated module loaded successfully\")\n",
    "    print(f\"Available components: {len(__all__)} classes and functions\")\n",
    "    print(\"Use help(nexus_fx_consolidated) for more information\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "18dcbb55",
    "outputId": "4be9de62-fc62-4fb9-d359-2dd3afc357b0"
   },
   "source": [
    "# import nexus_fx_consolidated as nfx # Removed this line\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Create configuration\n",
    "# You can customize these parameters based on your needs\n",
    "config = NexusFXConfig(\n",
    "    pairs=['EURUSD', 'GBPUSD', 'USDJPY', 'AUDUSD'],\n",
    "    batch_size=4, # Using a small batch size for demonstration\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=2, # Limiting epochs for quick demonstration\n",
    "    sequence_length=100, # Small sequence length for quick run\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# 2. Create dataset\n",
    "# For demonstration, we'll use synthetic data by not providing data_path.\n",
    "# In a real scenario, you would provide the path to your OHLCV CSV files.\n",
    "print(\"Generating synthetic dataset...\")\n",
    "full_dataset = ForexDataset(\n",
    "    data_path=None,  # Set to your data directory, e.g., './data'\n",
    "    pairs=config.pairs,\n",
    "    base_timeframe='5m',\n",
    "    target_timeframes=config.timeframes,\n",
    "    sequence_length=config.sequence_length,\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# 3. Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers if config.device == 'cpu' else 0 # Avoid multiprocessing with CUDA unless configured properly\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers if config.device == 'cpu' else 0\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# 4. Create model\n",
    "model = NEXUSFX(config)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# 5. Create trainer\n",
    "trainer = NexusFXTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=config.device\n",
    ")\n",
    "\n",
    "# 6. Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")"
   ],
   "id": "18dcbb55",
   "execution_count": 64,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cpu\n",
      "Generating synthetic dataset...\n",
      "Train dataset size: 7916\n",
      "Validation dataset size: 1979\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model has 16,619,209 parameters\n",
      "Starting training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0:   0%|          | 0/1979 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (100) at non-singleton dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-24455407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# 6. Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2538437350.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# Train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch} - Train Loss: {train_losses.get('total', 0):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2538437350.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {self.current_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2538437350.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Forward pass through the actual model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mohlc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ohlc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mvolume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-217219132.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ohlc, volume, timestamps, macro_data)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# Process through Titans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mtitans_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitans_per_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Take last timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1546399151.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, write_mode)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Process through layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mall_surprises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'surprise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mall_lrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3603114341.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, write_mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Use surprise as the target value for memory update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# This creates a self-supervised learning signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_to_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Generate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4086674082.py\u001b[0m in \u001b[0;36m_write_to_memory\u001b[0;34m(self, keys_new, values_new, surprise)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dgd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# DGD-style update: blend old and new with adaptive decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mupdated_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecay_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moldest_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkeys_to_write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mupdated_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecay_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moldest_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues_to_write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}